<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Alex Zwanenburg" />

<meta name="date" content="2022-12-16" />

<title>Feature selection methods</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<img src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   version="1.1"
   id="Layer_1"
   x="0px"
   y="0px"
   viewBox="0 0 735 852"
   style="enable-background:new 0 0 735 852;"
   xml:space="preserve"
   sodipodi:docname="familiar.svg"
   inkscape:version="1.0.1 (3bc2e813f5, 2020-09-07)"><metadata
   id="metadata79"><rdf:RDF><cc:Work
       rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type
         rdf:resource="http://purl.org/dc/dcmitype/StillImage" /></cc:Work></rdf:RDF></metadata><defs
   id="defs77"><clipPath
     clipPathUnits="userSpaceOnUse"
     id="clipPath1261"><path
       class="st0"
       d="M 18.2,626.4 V 223.1 L 69.8,193.3 367.5,21.5 v 0 l 296.7,171.3 52.6,30.3 V 626.4 L 367.5,828 Z"
       id="path1263"
       style="display:inline;fill:#ffd5e5" /></clipPath></defs><sodipodi:namedview
   pagecolor="#ffffff"
   bordercolor="#666666"
   borderopacity="1"
   objecttolerance="10"
   gridtolerance="10"
   guidetolerance="10"
   inkscape:pageopacity="0"
   inkscape:pageshadow="2"
   inkscape:window-width="2489"
   inkscape:window-height="1289"
   id="namedview75"
   showgrid="false"
   inkscape:zoom="0.70539906"
   inkscape:cx="455.24253"
   inkscape:cy="60.680079"
   inkscape:window-x="0"
   inkscape:window-y="0"
   inkscape:window-maximized="0"
   inkscape:current-layer="layer2" />
<style
   type="text/css"
   id="style2">
	.st0{fill:#144F85;}
	.st1{fill:#173E6C;}
	.st2{fill:#E6B35A;}
	.st3{fill:#FFFFFF;}
	.st4{fill:#231F20;}
</style>
<g
   inkscape:groupmode="layer"
   id="layer3"
   inkscape:label="Hexagon"
   style="display:inline"><path
     class="st0"
     d="M 18.2,626.4 V 223.1 L 69.8,193.3 367.5,21.5 v 0 l 296.7,171.3 52.6,30.3 V 626.4 L 367.5,828 Z"
     id="path4"
     style="fill:#ffd5e5" /><path
     class="st1"
     d="M 705.6,196.2 427.8,35.8 367.5,1 307.2,35.8 29.4,196.2 0.1,213.1 v 424.2 l 29.3,16.9 281.3,162.4 56.9,32.8 56.9,-32.8 281.3,-162.4 29.3,-16.9 V 213.1 Z m 0,425.3 L 367.5,816.7 29.4,621.5 V 231 L 367.5,35.8 v 0 L 705.6,231 Z"
     id="path8"
     style="fill:#ffaacc" /></g><g
   inkscape:groupmode="layer"
   id="layer2"
   inkscape:label="NetworkBG"><g
     id="g1203"
     clip-path="url(#clipPath1261)"><path
       style="fill:#ffeeaa;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 604.82727,123.80744 368.81604,260.06859 298.81945,261.12628 1.0461121,433.04579 29.3512,482.21648 327.12453,310.29697 397.12112,309.23928 633.13236,172.97813 Z"
       id="path1032-8"
       sodipodi:nodetypes="ccccccccc" /><path
       style="fill:#ffeeaa;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 524.66016,-15.335647 288.64893,120.9255 218.65234,121.98319 -79.120987,293.9027 -50.815883,343.07338 246.95744,171.15387 316.95403,170.09618 552.96527,33.835032 Z"
       id="path1032-8-8"
       sodipodi:nodetypes="ccccccccc" /><path
       style="fill:#ffdd55;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 552.96527,33.835032 316.95403,170.09618 246.95744,171.15387 -50.815883,343.07338 1.0461121,433.04579 298.81944,261.12628 368.81603,260.06859 604.82726,123.80744 Z"
       id="path1032-8-8-7"
       sodipodi:nodetypes="ccccccccc" /></g><path
     class="st1"
     d="M 705.6,196.2 427.8,35.8 367.5,1 307.2,35.8 29.4,196.2 0.1,213.1 v 424.2 l 29.3,16.9 281.3,162.4 56.9,32.8 56.9,-32.8 281.3,-162.4 29.3,-16.9 V 213.1 Z m 0,425.3 L 367.5,816.7 29.4,621.5 V 231 L 367.5,35.8 v 0 L 705.6,231 Z"
     id="path8-4"
     style="display:inline;fill:#ffaacc" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0"
     sodipodi:sides="5"
     sodipodi:cx="593.43592"
     sodipodi:cy="273.58523"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 628.55341,299.12523 -28.41819,-4.88808 -20.13738,20.6391 -4.13287,-28.5378 -25.85174,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43055,25.5168 z"
     inkscape:transform-center-x="4.1403908"
     inkscape:transform-center-y="-0.0063989586" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0-5"
     sodipodi:sides="5"
     sodipodi:cx="659.08881"
     sodipodi:cy="357.25101"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 694.2063,382.791 -28.41819,-4.88808 -20.13738,20.6391 -4.13286,-28.5378 -25.85175,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43054,25.5168 z"
     inkscape:transform-center-x="-2.3762241"
     inkscape:transform-center-y="-0.19639333"
     transform="matrix(0.50883698,0.35123436,-0.35123436,0.50883698,437.9071,-51.238505)" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0-5-4"
     sodipodi:sides="5"
     sodipodi:cx="659.08881"
     sodipodi:cy="357.25101"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 694.2063,382.791 -28.41819,-4.88808 -20.13738,20.6391 -4.13286,-28.5378 -25.85175,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43054,25.5168 z"
     inkscape:transform-center-x="-2.3762241"
     inkscape:transform-center-y="-0.19639333"
     transform="matrix(0.50883698,0.35123436,-0.35123436,0.50883698,458.27634,-128.13567)" /></g><g
   inkscape:groupmode="layer"
   id="layer1"
   inkscape:label="Familiar"
   style="display:inline"
   sodipodi:insensitive="true"><path
     id="rect1265"
     style="fill:#ffd5e5;stroke:none;stroke-width:0.888669;stroke-linecap:round;stroke-linejoin:round"
     d="m 245.46055,257.17563 229.95737,-15.59401 25.54911,67.75725 -0.0316,100.16657 -113.21601,42.22457 -148.63819,-42.93339 z"
     sodipodi:nodetypes="ccccccc" /><path
     style="fill:#d40055;stroke:none;stroke-width:3.16295"
     d="m 196.18676,315.6231 c -10.63253,17.38577 -33.64223,11.64186 -43.00737,28.79103 -36.61508,67.04939 17.43539,145.00334 62.13877,186.97879 17.5006,16.4315 42.7508,54.66573 50.734,46.89383 7.9832,-7.7719 -15.8147,-44.28123 -15.8147,-44.28123 28.2966,-20.2857 82.299,43.82139 86.6834,19.30534 4.3845,-24.51606 -46.8635,-18.95742 -67.7058,-44.60891 18.3594,0.003 59.6957,1.2462 63.2589,-9.48884 3.5633,-10.73504 -72.7477,-12.65178 -72.7477,-12.65178 v -9.48883 c 34.0706,-9.2959 57.6399,0.0886 91.7254,-0.81921 40.6388,-1.08172 68.0795,-15.63444 104.3772,7.1451 -25.2055,6.06969 -72.3313,25.48918 -69.5848,31.62945 2.7465,6.14027 72.7477,-3.16294 72.7477,-3.16294 0,0 -57.9896,56.7778 -37.9553,63.25891 20.0343,6.48111 47.0025,-53.26957 69.2495,-46.34981 18.3641,5.25049 12.4794,48.94683 25.6389,46.34981 13.1595,-2.59702 -1.3633,-35.41234 4.7665,-53.77008 15.9318,-47.71619 71.4573,-124.46633 44.8348,-177.06643 -9.8589,-19.48185 -35.2004,-18.55479 -45.9956,-38.02841 -10.0929,-18.20718 -8.0181,-37.2576 -22.925,-53.74319 -34.35974,-24.66838 -39.39345,-40.50377 -107.1986,-44.29358 0,0 -79.32687,1.91436 -115.39694,20.37701 -36.07006,18.46265 -63.11016,62.96566 -77.82326,87.02397 z m 288.1086,-44.14206 c -31.1126,3.20944 -51.2432,56.52436 -12.6391,55.74438 9.2389,-0.18661 17.684,-6.95247 25.2909,-11.46315 14.9228,44.08514 -20.2808,63.4528 -60.096,71.8422 -66.9715,14.1118 -129.6121,21.60703 -189.7767,-18.07212 4.9516,-26.65573 -6.3679,-90.45044 15.8758,-103.05699 11.5245,-6.53148 25.0878,-7.94627 37.6087,-3.39226 16.6016,6.03838 24.9053,24.81679 41.4064,32.38129 36.5393,16.75032 91.7428,-5.26188 88.5599,-49.28692 22.9719,0 49.3293,-3.21766 53.7701,25.30357 m -205.2498,18.1746 c -31.0111,12.78146 -7.1581,66.184 24.286,52.66209 29.17,-12.54392 6.5498,-65.37112 -24.286,-52.66209 m 132.5021,41.92136 0.8771,11.7139 8.3914,-0.0645 -1.0516,-13.03381 -8.2169,1.38445 m -50.6071,3.16295 3.1629,12.65178 h 6.3259 l 3.1629,-12.65178 h -12.6517 m 142.3325,50.60712 c -51.7831,62.49348 -152.6773,51.3441 -221.4062,34.79241 0,0 79.3233,3.56324 116.6478,-2.66733 37.3245,-6.23056 104.7584,-32.12508 104.7584,-32.12508 m -53.7701,75.9107 c 7.8662,-5.63637 54.0041,-39.90056 58.1855,-37.39867 10.656,6.37966 -2.9763,25.22449 -8.2552,30.35163 -13.2433,12.85737 -33.4608,11.91481 -49.9303,7.04704 m -192.9396,-6.32589 v 6.32589 c -22.296,10.71289 -35.7552,2.09071 -37.9554,-22.14062 z"
     id="path988"
     sodipodi:nodetypes="scczczczcccczczczccscczscscccssscccsccccccccccccczccccccccc" /><text
     xml:space="preserve"
     style="font-size:106.667px;line-height:1.25;font-family:Bahnschrift;-inkscape-font-specification:Bahnschrift;fill:#ffffff"
     x="133.01973"
     y="669.02026"
     id="text1007"><tspan
       sodipodi:role="line"
       id="tspan1005"
       x="133.01973"
       y="669.02026"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:106.667px;font-family:'Lucida Sans';-inkscape-font-specification:'Lucida Sans';fill:#ffffff">FAMILIAR</tspan></text><path
     style="fill:#ffaacc;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
     d="M 362.57822,85.512104 311.7313,246.86829 c 0,0 25.2203,31.68457 54.88255,30.82445 36.78744,-1.06673 54.26709,-35.12035 54.26709,-35.12035 z"
     id="path1009"
     sodipodi:nodetypes="ccscc" /><path
     sodipodi:type="star"
     style="fill:#ffe680;stroke:none;stroke-width:0.463344;stroke-linecap:round;stroke-linejoin:round"
     id="path1011"
     sodipodi:sides="5"
     sodipodi:cx="393.43726"
     sodipodi:cy="240.33679"
     sodipodi:r1="21.630606"
     sodipodi:r2="10.815303"
     sodipodi:arg1="1.300471"
     sodipodi:arg2="1.9287895"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 399.2136,261.18186 -9.56598,-10.71543 -14.25022,1.80547 7.23494,-12.40903 -6.12067,-12.99484 14.03741,3.04623 10.46744,-9.83673 1.44066,14.29171 12.5899,6.9154 -13.14703,5.78653 z"
     inkscape:transform-center-x="-1.7849856"
     inkscape:transform-center-y="0.29290531" /><path
     sodipodi:type="star"
     style="fill:#ffe680;stroke:none;stroke-width:0.552007;stroke-linecap:round;stroke-linejoin:round"
     id="path1013"
     sodipodi:sides="5"
     sodipodi:cx="365.95331"
     sodipodi:cy="171.80791"
     sodipodi:r1="24.417637"
     sodipodi:r2="12.208818"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 385.70073,186.16967 -15.98024,-2.74869 -11.32374,11.60587 -2.32401,-16.0475 -14.53707,-7.18311 14.54393,-7.16921 2.33934,-16.04528 11.31265,11.61668 15.98286,-2.73341 -7.55232,14.34872 z"
     inkscape:transform-center-x="2.3282526"
     inkscape:transform-center-y="-0.0036080646" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:none;stroke-width:0.440145;stroke-linecap:round;stroke-linejoin:round"
     id="path1011-6"
     sodipodi:sides="5"
     sodipodi:cx="344.4216"
     sodipodi:cy="225.49474"
     sodipodi:r1="20.547558"
     sodipodi:r2="10.273779"
     sodipodi:arg1="1.300471"
     sodipodi:arg2="1.9287895"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 349.90872,245.29609 -9.087,-10.17891 -13.53671,1.71507 6.87268,-11.78771 -5.81421,-12.34419 13.33456,2.89371 9.94333,-9.34421 1.36853,13.57613 11.95952,6.56915 -12.48876,5.4968 z"
     inkscape:transform-center-x="-1.6956102"
     inkscape:transform-center-y="0.27822403" /></g>
<title
   id="title6">feather</title>

































</svg>
" align="right" width="120" />

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Feature selection methods</h1>
<h4 class="author">Alex Zwanenburg</h4>
<h4 class="date">2022-12-16</h4>


<div id="TOC">
<ul>
<li><a href="#configuration-options" id="toc-configuration-options">Configuration options</a></li>
<li><a href="#providing-parameters-for-feature-selection" id="toc-providing-parameters-for-feature-selection">Providing parameters
for feature selection</a></li>
<li><a href="#overview-of-feature-selection-methods" id="toc-overview-of-feature-selection-methods">Overview of feature
selection methods</a>
<ul>
<li><a href="#correlation-methods" id="toc-correlation-methods">Correlation methods</a></li>
<li><a href="#concordance-methods" id="toc-concordance-methods">Concordance methods</a></li>
<li><a href="#corelearn-methods" id="toc-corelearn-methods">CORElearn
methods</a></li>
<li><a href="#mutual-information-based-methods" id="toc-mutual-information-based-methods">Mutual information-based
methods</a>
<ul>
<li><a href="#mutual-information-maximisation" id="toc-mutual-information-maximisation">Mutual information
maximisation</a></li>
<li><a href="#mutual-information-feature-selection" id="toc-mutual-information-feature-selection">Mutual information feature
selection</a></li>
<li><a href="#minimum-redundancy-maximum-relevance" id="toc-minimum-redundancy-maximum-relevance">Minimum redundancy maximum
relevance</a></li>
</ul></li>
<li><a href="#univariate-and-multivariate-regression-methods" id="toc-univariate-and-multivariate-regression-methods">Univariate and
multivariate regression methods</a>
<ul>
<li><a href="#univariate-regression" id="toc-univariate-regression">Univariate regression</a></li>
<li><a href="#multivariate-regression" id="toc-multivariate-regression">Multivariate regression</a></li>
</ul></li>
<li><a href="#lasso-ridge-and-elastic-net-regression" id="toc-lasso-ridge-and-elastic-net-regression">Lasso, ridge and elastic
net regression</a></li>
<li><a href="#random-forest-based-methods" id="toc-random-forest-based-methods">Random forest-based methods</a>
<ul>
<li><a href="#permutation-importance" id="toc-permutation-importance">Permutation importance</a></li>
<li><a href="#holdout-permutation-importance" id="toc-holdout-permutation-importance">Holdout permutation
importance</a></li>
<li><a href="#minimum-depth-variable-selection" id="toc-minimum-depth-variable-selection">Minimum depth variable
selection</a></li>
<li><a href="#variable-hunting" id="toc-variable-hunting">Variable
hunting</a></li>
<li><a href="#impurity-importance" id="toc-impurity-importance">Impurity
importance</a></li>
</ul></li>
<li><a href="#special-methods" id="toc-special-methods">Special
methods</a>
<ul>
<li><a href="#no-feature-selection" id="toc-no-feature-selection">No
feature selection</a></li>
<li><a href="#random-feature-selection" id="toc-random-feature-selection">Random feature selection</a></li>
<li><a href="#signature-only" id="toc-signature-only">Signature
only</a></li>
</ul></li>
</ul></li>
<li><a href="#aggregating-variable-importance" id="toc-aggregating-variable-importance">Aggregating variable
importance</a>
<ul>
<li><a href="#notation" id="toc-notation">Notation</a></li>
<li><a href="#no-rank-aggregation" id="toc-no-rank-aggregation">No rank
aggregation</a></li>
<li><a href="#mean-rank-aggregation" id="toc-mean-rank-aggregation">Mean
rank aggregation</a></li>
<li><a href="#median-rank-aggregation" id="toc-median-rank-aggregation">Median rank aggregation</a></li>
<li><a href="#best-rank-aggregation" id="toc-best-rank-aggregation">Best
rank aggregation</a></li>
<li><a href="#worst-rank-aggregation" id="toc-worst-rank-aggregation">Worst rank aggregation</a></li>
<li><a href="#stability-rank-aggregation" id="toc-stability-rank-aggregation">Stability rank aggregation</a></li>
<li><a href="#exponential-rank-aggregation" id="toc-exponential-rank-aggregation">Exponential rank
aggregation</a></li>
<li><a href="#borda-rank-aggregation" id="toc-borda-rank-aggregation">Borda rank aggregation</a></li>
<li><a href="#enhanced-borda-rank-aggregation" id="toc-enhanced-borda-rank-aggregation">Enhanced borda rank
aggregation</a></li>
<li><a href="#truncated-borda-rank-aggregation" id="toc-truncated-borda-rank-aggregation">Truncated borda rank
aggregation</a></li>
<li><a href="#truncated-enhanced-borda-rank-aggregation" id="toc-truncated-enhanced-borda-rank-aggregation">Truncated enhanced
borda rank aggregation</a></li>
</ul></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>Feature selection methods in familiar measure variable importance in
a univariate or multivariate setting.</p>
<table>
<caption>Overview of feature selection methods. <sup>a</sup> This is a
general method where an appropriate specific method will be chosen, or
multiple distributions or linking families are tested in an attempt to
find the best option. <sup>b</sup> This method requires hyperparameter
optimisation.</caption>
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>method</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>binomial</strong></th>
<th align="center"><strong>multinomial</strong></th>
<th align="center"><strong>continuous</strong></th>
<th align="center"><strong>count</strong></th>
<th align="center"><strong>survival</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>correlation</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">Pearson’s <em>r</em></td>
<td align="left"><code>pearson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">Spearman’s <em>ρ</em></td>
<td align="left"><code>spearman</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">Kendall’s <em>τ</em></td>
<td align="left"><code>kendall</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>concordance</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">concordance<sup>a</sup></td>
<td align="left"><code>concordance</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>CORElearn</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">information gain ratio</td>
<td align="left"><code>gain_ratio</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">gini-index</td>
<td align="left"><code>gini</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">minimum description length</td>
<td align="left"><code>mdl</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">ReliefF with exponential weighting of distance
ranks</td>
<td align="left"><code>relieff_exp_rank</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><strong>mutual information</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">mutual information maximisation</td>
<td align="left"><code>mim</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">mutual information features selection</td>
<td align="left"><code>mifs</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">minimum redundancy maximum relevance</td>
<td align="left"><code>mrmr</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>univariate regression</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">univariate regression</td>
<td align="left"><code>univariate_regression</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>multivariate regression</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">multivariate regression</td>
<td align="left"><code>multivariate_regression</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>lasso regression</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>lasso</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">logistic</td>
<td align="left"><code>lasso_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">multi-logistic</td>
<td align="left"><code>lasso_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">normal (gaussian)</td>
<td align="left"><code>lasso_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">poisson</td>
<td align="left"><code>lasso_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cox</td>
<td align="left"><code>lasso_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>ridge regression</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>ridge</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>ridge_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">multi-logistic</td>
<td align="left"><code>ridge_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">normal (gaussian)</td>
<td align="left"><code>ridge_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>ridge_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>ridge_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>elastic net regression</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">general<sup>a,b</sup></td>
<td align="left"><code>elastic_net</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">logistic<sup>b</sup></td>
<td align="left"><code>elastic_net_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">multi-logistic<sup>b</sup></td>
<td align="left"><code>elastic_net_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">normal (gaussian)<sup>b</sup></td>
<td align="left"><code>elastic_net_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">poisson<sup>b</sup></td>
<td align="left"><code>elastic_net_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cox<sup>b</sup></td>
<td align="left"><code>elastic_net_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>random forest (RFSRC) variable
importance</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">permutation<sup>b</sup></td>
<td align="left"><code>random_forest_permutation</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">permutation (unoptimised)</td>
<td align="left"><code>random_forest_permutation_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">minimum depth<sup>b</sup></td>
<td align="left"><code>random_forest_minimum_depth</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">minimum depth (unoptimised)</td>
<td align="left"><code>random_forest_minimum_depth_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">variable hunting<sup>b</sup></td>
<td align="left"><code>random_forest_variable_hunting</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">variable hunting (unoptimised)</td>
<td align="left"><code>random_forest_variable_hunting_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">hold-out <sup>b</sup></td>
<td align="left"><code>random_forest_holdout</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">hold-out (unoptimised)</td>
<td align="left"><code>random_forest_holdout_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>random forest (ranger) variable
importance</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">permutation<sup>b</sup></td>
<td align="left"><code>random_forest_ranger_permutation</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">permutation (unoptimised)</td>
<td align="left"><code>random_forest_ranger_permutation_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">hold-out permutation<sup>b</sup></td>
<td align="left"><code>random_forest_ranger_holdout_permutation</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">hold-out perm. (unoptimised)</td>
<td align="left"><code>random_forest_ranger_holdout_permutation_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">impurity<sup>b</sup></td>
<td align="left"><code>random_forest_ranger_impurity</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">x</td>
</tr>
<tr class="even">
<td align="left">impurity (unoptimised)</td>
<td align="left"><code>random_forest_ranger_impurity_default</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">x</td>
</tr>
<tr class="odd">
<td align="left"><strong>special methods</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">no selection</td>
<td align="left"><code>none</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">random selection</td>
<td align="left"><code>random</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">signature only</td>
<td align="left"><code>signature_only</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
</tbody>
</table>
<div id="configuration-options" class="section level2">
<h2>Configuration options</h2>
<p>Feature selection methods and related options can be provided within
the <code>feature_selection</code> tag in the <em>xml</em> file or as
function argument.</p>
<table>
<colgroup>
<col width="23%" />
<col width="50%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><strong>tag</strong> / <strong>argument</strong></th>
<th align="left"><strong>description</strong></th>
<th align="center"><strong>default</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>fs_method</code></td>
<td align="left">The desired feature selection method. Multiple
selection methods may be provided at the same time. This setting has no
default and must be provided.</td>
<td align="center">– (required)</td>
</tr>
<tr class="even">
<td align="center"><code>fs_method_parameter</code></td>
<td align="left">Several feature selection methods have hyperparameters
that can be set and/or optimised.</td>
<td align="center">– (optional)</td>
</tr>
<tr class="odd">
<td align="center"><code>vimp_aggregation_method</code></td>
<td align="left">The aggregation method used to aggregate feature ranks
over different bootstraps.</td>
<td align="center"><code>borda</code></td>
</tr>
<tr class="even">
<td align="center"><code>vimp_aggregation_rank_threshold</code></td>
<td align="left">Several aggregation methods count features if they have
a rank below the threshold, i.e. are among the most important features.
If <code>NULL</code>, a dynamic threshold is decided through
Otsu-thresholding.</td>
<td align="center"><code>NULL</code></td>
</tr>
<tr class="odd">
<td align="center"><code>parallel_feature_selection</code></td>
<td align="left">Enables parallel processing for feature selection.
Ignored if <code>parallel=FALSE</code>.</td>
<td align="center"><code>TRUE</code></td>
</tr>
</tbody>
</table>
</div>
<div id="providing-parameters-for-feature-selection" class="section level2">
<h2>Providing parameters for feature selection</h2>
<p>Some of the feature selection methods, notably those based on random
forests and (penalised) regression, have parameters that can be set.
These parameters are mentioned under the respective entries in the <a href="#overview-of-feature-selection-methods">Overview of feature
selection methods</a> section. Moreover, some of these parameters are
model parameters. In this case, these parameters are optimised using
hyperparameter optimisation, which is described in the <em>learning
algorithms and hyperparameter optimisation</em> vignette.</p>
<p>The syntax for such parameters is the same as for hyperparameter
optimisation. For the <code>multivariate_regression</code> feature
selection method the <code>alpha</code> parameter (which determines
feature drop-out during forward selection) may be provided as follows
using the configuration file:</p>
<pre><code>&lt;fs_method_parameter&gt;
  &lt;multivariate_regression&gt;
    &lt;alpha&gt;0.05&lt;/alpha&gt;
  &lt;/multivariate_regression&gt;
&lt;/fs_method_parameter&gt;</code></pre>
<p>Or as a nested list passed as the <code>fs_method_parameter</code>
argument to <code>summon_familiar</code>:</p>
<pre><code>fs_method_parameter = list(&quot;multivariate_regression&quot;=list(&quot;alpha&quot;=0.05))</code></pre>
</div>
<div id="overview-of-feature-selection-methods" class="section level1">
<h1>Overview of feature selection methods</h1>
<p>The feature selection methods implemented in familiar are described
in more detail in this section.</p>
<div id="correlation-methods" class="section level2">
<h2>Correlation methods</h2>
<p>Correlation methods determine variable importance by assessing the
correlation between a feature and the outcome of interest. High
(anti-)correlation indicates an important feature, whereas low
(anti-)correlation indicates that a feature is not directly related to
the outcome. Correlation-based variable importance is determined using
the <code>cor</code> function of the <code>stats</code> package that is
part of the R core distribution <span class="citation">(R Core Team
2019)</span>.</p>
<p>Three correlation coefficients can be computed:</p>
<ul>
<li><code>pearson</code>: Pearson’s <span class="math inline">\(r\)</span></li>
<li><code>spearman</code>: Spearman’s <span class="math inline">\(\rho\)</span></li>
<li><code>kendall</code>: Kendall’s <span class="math inline">\(\tau\)</span></li>
</ul>
<p>To compute correlation of features with survival outcomes, only
samples with an event are considered.</p>
</div>
<div id="concordance-methods" class="section level2">
<h2>Concordance methods</h2>
<p>Concordance methods assess how well the ordering of feature values
corresponds to the ordering of the outcome. The method internally refers
to the <code>gini</code> method for binomial and multinomial outcomes
and to the <code>kendall</code> method for continuous and count
outcomes. For survival outcomes, concordance is measured using the
<code>concordance_index</code>.</p>
</div>
<div id="corelearn-methods" class="section level2">
<h2>CORElearn methods</h2>
<p>Familiar provides an interface to several feature selection methods
implemented in the <code>CORElearn</code> package. These methods are the
Information Gain Ratio (<code>gain_ratio</code>), the Gini-index
(<code>gini</code>), Minimum Description Length (<code>mdl</code>) and
ReliefF and rReliefF with exponential distance rank weighting
(<code>relieff_exp_rank</code>).</p>
</div>
<div id="mutual-information-based-methods" class="section level2">
<h2>Mutual information-based methods</h2>
<p>Mutual information <span class="math inline">\(I\)</span> is a
measure of interdependency between two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. In the context of feature selection,
<span class="math inline">\(x\)</span> is a feature vector and <span class="math inline">\(y\)</span> is the outcome vector.</p>
<p>Computing mutual information requires that probability distributions
of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are known. In practice we don’t know
either one. For categorical <span class="math inline">\(x\)</span> and
<span class="math inline">\(y\)</span> we can use the sample estimates
instead. For continuous or mixed data, the situation is more
complex.</p>
<p>In familiar we therefore use the following three approaches to
compute mutual information:</p>
<ol style="list-style-type: decimal">
<li><p>For binomial and multinomial outcomes mutual information is
computed using sample estimates. In case of continuous <span class="math inline">\(x\)</span>, these are discretised into <span class="math inline">\(\lceil 2 n^{1/3} \rceil\)</span> bins, with <span class="math inline">\(n\)</span> the number of samples, after which
computation is conducted as if <span class="math inline">\(x\)</span>
was a categorical variable.</p></li>
<li><p>For continuous and count outcomes, we use the approximation
proposed by De Jay et al. after Gel’fand and Yaglom <span class="citation">(Gel′fand and Yaglom 1959; De Jay et al. 2013)</span>:
<span class="math inline">\(I = -0.5 \log(1 - \rho(x,y)^2 +
\epsilon)\)</span>, with <span class="math inline">\(\rho(x,y)\)</span>
Spearman’s correlation coefficient and <span class="math inline">\(\epsilon\)</span> a small positive number to
prevent <span class="math inline">\(\log(0)\)</span>.</p></li>
<li><p>For survival outcomes the second method is adapted for use with a
concordance index: <span class="math inline">\(I = -0.5 \log(1 - (2 *
(ci-0.5))^2 + \epsilon)\)</span>, with <span class="math inline">\(ci\)</span> the concordance index.</p></li>
</ol>
<p>We opted to adapt the approach based on the outcome type as this
ensures that a single consistent approach is used to assess all feature
data in an analysis, thus making results comparable.</p>
<div id="mutual-information-maximisation" class="section level3">
<h3>Mutual information maximisation</h3>
<p>The <code>mim</code> method is a univariate method that ranks each
feature by its mutual information with the outcome.</p>
</div>
<div id="mutual-information-feature-selection" class="section level3">
<h3>Mutual information feature selection</h3>
<p>Mutual information feature selection (MIFS) finds a feature set that
maximises mutual information <span class="citation">(Battiti
1994)</span>. This is done using forward selection. As in mutual
information maximisation, mutual information <span class="math inline">\(I_{y,j}\)</span> between each feature and the
outcome is computed. Starting from a potential pool of all features, the
feature with the highest mutual information is selected and removed from
the pool.</p>
<p>The rest proceeds iteratively. The mutual information <span class="math inline">\(I_{s,1j}\)</span> between the previously selected
feature and the remaining features is computed. This mutual information
is also called <em>redundancy</em>. The feature with the highest mutual
information with the outcome and least redundancy (i.e. maximum <span class="math inline">\(I_{y,j} - I_{s,1j}\)</span>) is selected next, and
removed from the pool of remaining features. Then the mutual information
<span class="math inline">\(I_{s,2j}\)</span> between this feature and
remaining features is computed, and the feature that maximises <span class="math inline">\(I_{y,j} - I{s,1j} - I_{s,2j}\)</span> is selected,
and so forth.</p>
<p>The iterative process stops if there is no feature <span class="math inline">\(j\)</span> for which <span class="math inline">\(I_{y,j} - \sum_{i\in S} I_{s,ij} &gt; 0\)</span>,
with <span class="math inline">\(S\)</span> being the subset of selected
features, or all features have been exhausted.</p>
<p>To reduce the number of required computations, the implementation in
familiar actively filters out any feature <span class="math inline">\(j\)</span> for which <span class="math inline">\(I_{y,j} - \sum_{i\in S} I_{s,ij} \leq 0\)</span>
at the earliest instance, as the <span class="math inline">\(\sum_{i\in
S} I_{s,ij}\)</span> term will monotonously increase.</p>
</div>
<div id="minimum-redundancy-maximum-relevance" class="section level3">
<h3>Minimum redundancy maximum relevance</h3>
<p>Minimum redundancy maximum relevance (mRMR) feature selection is
similar to MIFS but differs in the way redundancy is used during
optimisation <span class="citation">(Peng, Long, and Ding 2005)</span>.
Whereas for MIFS the optimisation criterion is <span class="math inline">\(I_{y,j} - \sum_{i\in S} I_{s,ij}\)</span>, in mRMR
the optimisation criterion is <span class="math inline">\(I_{y,j} -
\frac{1} {\left| S \right|} \sum_{i\in S} I_{s,ij}\)</span>, with <span class="math inline">\(\left| S \right|\)</span> the number of features
already selected.</p>
<p>Unlike in MIFS, the <span class="math inline">\(\frac{1}{\left|S\right|}\sum_{i\in
S}I_{s,ij}\)</span> term is not monotonically increasing. Consequently,
features cannot be safely filtered. To limit computational complexity,
we still remove features for which <span class="math inline">\(I_{y,j} -
\frac{1} {\left| S \right| + 3} \sum_{i\in S} I_{s,ij} \leq 0\)</span>,
as such features are unlikely to be selected.</p>
</div>
</div>
<div id="univariate-and-multivariate-regression-methods" class="section level2">
<h2>Univariate and multivariate regression methods</h2>
<p>Univariate and multivariate regression perform feature selection by
performing regression using a feature or set of features as predictors.
The performance of the regression model is then measured using a metric.
Training and testing of regression models are repeated multiple times
using bootstraps. For each bootstrap, the in-bag samples are used for
training and the out-of-bag samples are using for testing.</p>
<p>This also defines the parameters of both methods, which are shown in
the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">regression learner</td>
<td align="left"><code>learner</code></td>
<td align="center">dependent on outcome</td>
<td align="center">no</td>
<td align="left">Any generalised linear regression model from the
<em>learning algorithms and hyperparameter optimisation</em> vignette
can be selected. Default values are <code>glm_logistic</code> for
binomial, <code>glm_multinomial</code> for multinomial,
<code>glm_gaussian</code> for continuous, <code>glm_poisson</code> for
count, and <code>cox</code> for survival outcomes.</td>
</tr>
<tr class="even">
<td align="left">performance metric</td>
<td align="left"><code>metric</code></td>
<td align="center">dependent on outcome</td>
<td align="center">no</td>
<td align="left">Any metric from the <em>performance metrics</em>
vignette can be selected. Default values are <code>auc_roc</code> for
binomial and multinomial, <code>mse</code> for continuous,
<code>msle</code> for count and <code>concordance_index</code> for
survival outcomes</td>
</tr>
<tr class="odd">
<td align="left">number of bootstraps</td>
<td align="left"><code>n_bootstrap</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">The default value is <span class="math inline">\(10\)</span>.</td>
</tr>
<tr class="even">
<td align="left">drop-out alpha level</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0,
1\right]\)</span></td>
<td align="center">no</td>
<td align="left">The default value is <span class="math inline">\(0.05\)</span>. Only used in multivariate
regression.</td>
</tr>
</tbody>
</table>
<div id="univariate-regression" class="section level3">
<h3>Univariate regression</h3>
<p>In the univariate regression method, a regression model is built with
each feature separately using the in-bag data of the bootstrap. Then
this model is evaluated using the metric, expressed using an objective
representation (see <em>computing the objective score</em> in the
<em>learning algorithms and hyperparameter optimisation</em> vignette).
The objective representation <span class="math inline">\(s^*\)</span> is
computed on both in-bag (IB) and out-of-bag (OOB) data. Subsequently the
<code>balanced</code> objective score <span class="math inline">\(f\)</span> is computed: <span class="math inline">\(f=s^*_{OOB} -
\left|s^*_{OOB}-s^*_{IB}\right|\)</span>.</p>
<p>The objective score <span class="math inline">\(f\)</span> is
subsequently averaged over all bootstraps to obtain the variable
importance of a feature.</p>
</div>
<div id="multivariate-regression" class="section level3">
<h3>Multivariate regression</h3>
<p>The procedure described for univariate regression forms the first
step in multivariate regression. The rest follows forward selection. The
most important feature is assigned to the subset of selected features
and removed from the set of available features. Separate regression
models are then built with each remaining feature and all the feature(s)
in the selected feature subset as predictors. Thus, the subset of
selected features iteratively increases in size until no features are
remaining or the objective score no longer increases.</p>
<p>To limit mostly redundant computation, features that are unlikely to
be selected are actively removed. To do so, the standard deviation of
the objective score over the bootstraps is computed for each feature.
The (one-sided, upper-tail) quantile <span class="math inline">\(q\)</span> corresponding to the alpha-level
indicated by parameter <code>alpha</code> is subsequently computed. If
the obtained mean objective score is <span class="math inline">\(q\)</span> standard deviations or more below the
best objective score, the feature is removed.</p>
</div>
</div>
<div id="lasso-ridge-and-elastic-net-regression" class="section level2">
<h2>Lasso, ridge and elastic net regression</h2>
<p>Penalised regression is also a form of feature selection, as it
selects an ‘optimal’ set of features to create a regression model. As
features are usually normalised as part of pre-processing, the magnitude
of each coefficient can be interpreted as its importance. All three
shrinkage methods are implemented using the <code>glmnet</code> package
<span class="citation">(Hastie, Tibshirani, and Friedman 2009; Simon et
al. 2011)</span>.</p>
<p>Only elastic net regression has a model hyperparameter that requires
optimisation, but other parameters may be set as well, as shown in the
table below:</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">family</td>
<td align="left"><code>family</code></td>
<td align="center"><code>gaussian</code>, <code>binomial</code>,
<code>poisson</code>, <code>multinomial</code>, <code>cox</code></td>
<td align="center">continuous outcomes</td>
<td align="left">For continuous outcomes <code>gaussian</code> and
<code>poisson</code> may be tested. The family is not optimised when it
is specified, e.g. <code>lasso_gaussian</code>. For other outcomes only
one applicable family exists.</td>
</tr>
<tr class="even">
<td align="left">elastic net penalty</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,1\right]\)</span></td>
<td align="center">elastic net</td>
<td align="left">This penalty is fixed for ridge regression
(<code>alpha = 0</code>) and lasso (<code>alpha = 1</code>).</td>
</tr>
<tr class="odd">
<td align="left">optimal lambda</td>
<td align="left"><code>lambda_min</code></td>
<td align="center"><code>lambda.1se</code>, <code>lambda.min</code></td>
<td align="center">no</td>
<td align="left">Default is <code>lambda.min</code>.</td>
</tr>
<tr class="even">
<td align="left">number of CV folds</td>
<td align="left"><code>n_folds</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[3,n\right]\)</span></td>
<td align="center">no</td>
<td align="left">Default is <span class="math inline">\(3\)</span> if
<span class="math inline">\(n&lt;30\)</span>, <span class="math inline">\(\lfloor n/10\rfloor\)</span> if <span class="math inline">\(30\leq n \leq 200\)</span> and <span class="math inline">\(20\)</span> if <span class="math inline">\(n&gt;200\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">normalisation</td>
<td align="left"><code>normalise</code></td>
<td align="center"><code>FALSE</code>, <code>TRUE</code></td>
<td align="center">no</td>
<td align="left">Default is <code>FALSE</code>, as normalisation is part
of pre-processing in familiar.</td>
</tr>
</tbody>
</table>
</div>
<div id="random-forest-based-methods" class="section level2">
<h2>Random forest-based methods</h2>
<p>Several feature selection methods are based on random forests. All
these methods require that a random forest model exists. Hence,
<code>familiar</code> will train a random forest based on the training
data. Random forest learners have a set of hyperparameters that are
optimised prior to training, and these make up most of the
method-specific parameters. These parameters, which are slightly
different for <code>ranger</code>-based and
<code>randomForestSRC</code>-based methods, are shown below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">number of trees</td>
<td align="left"><code>n_tree</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[0,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{2}\)</span> scale, i.e. the actual input
value will be <span class="math inline">\(2^\texttt{n_tree}\)</span>
<span class="citation">(Oshiro, Perez, and Baranauskas 2012)</span>. The
default range is <span class="math inline">\(\left[4,
10\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">subsampling fraction</td>
<td align="left"><code>sample_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Fraction of available data that is used for to create a
single tree. The default range is <span class="math inline">\(\left[2 /
m, 1.0\right]\)</span>, with <span class="math inline">\(m\)</span> the
number of samples.</td>
</tr>
<tr class="odd">
<td align="left">number of features at each node</td>
<td align="left"><code>m_try</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0.0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Familiar ensures that there is always at least one
candidate feature.</td>
</tr>
<tr class="even">
<td align="left">node size</td>
<td align="left"><code>node_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Minimum number of unique samples in terminal nodes. The
default range is <span class="math inline">\(\left[5, \lfloor m /
3\rfloor\right]\)</span>, with <span class="math inline">\(m\)</span>
the number of samples.</td>
</tr>
<tr class="odd">
<td align="left">maximum tree depth</td>
<td align="left"><code>tree_depth</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Maximum depth to which trees are allowed to grow. The
default range is <span class="math inline">\(\left[1,
10\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">number of split points</td>
<td align="left"><code>n_split</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[0,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">By default, splitting is deterministic and has one
split point (<span class="math inline">\(0\)</span>).</td>
</tr>
<tr class="odd">
<td align="left">splitting rule (<code>randomForestSRC</code> only)</td>
<td align="left"><code>split_rule</code></td>
<td align="center"><code>gini</code>, <code>auc</code>,
<code>entropy</code>, <code>mse</code>, <code>quantile.regr</code>,
<code>la.quantile.regr</code>, <code>logrank</code>,
<code>logrankscore</code>, <code>bs.gradient</code></td>
<td align="center">no</td>
<td align="left">Default splitting rules are <code>gini</code> for
<code>binomial</code> and <code>multinonial</code> outcomes,
<code>mse</code> for <code>continuous</code> and <code>count</code>
outcomes and <code>logrank</code> for <code>survival</code>
outcomes.</td>
</tr>
<tr class="even">
<td align="left">splitting rule (<code>ranger</code> only)</td>
<td align="left"><code>split_rule</code></td>
<td align="center"><code>gini</code>, <code>hellinger</code>,
<code>extratrees</code>, <code>beta</code>, <code>variance</code>,
<code>logrank</code>, <code>C</code>, <code>maxstat</code></td>
<td align="center">no</td>
<td align="left">Default splitting rules are <code>gini</code> for
<code>binomial</code> and <code>multinomial</code> outcomes and
<code>maxstat</code> for <code>continuous</code>, <code>count</code> and
<code>survival</code> outcomes.</td>
</tr>
<tr class="odd">
<td align="left">significance split threshold (<code>ranger</code>
only)</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0.0,
1.0\right]\)</span></td>
<td align="center"><code>maxstat</code></td>
<td align="left">Minimum significance level for further splitting. The
default range is <span class="math inline">\(\left[10^{-6},
1.0\right]\)</span></td>
</tr>
<tr class="even">
<td align="left">variable hunting cross-validation folds</td>
<td align="left"><code>fs_vh_fold</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[2,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">Number of cross-validation folds for the
<code>random_forest_variable_hunting</code> method. The default is <span class="math inline">\(5\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">variable hunting step size</td>
<td align="left"><code>fs_vh_step_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">Step size for the
<code>random_forest_variable_hunting</code> method. The default is <span class="math inline">\(1\)</span>.</td>
</tr>
<tr class="even">
<td align="left">variable hunting iterations</td>
<td align="left"><code>fs_vh_n_rep</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">Number of Monte Carlo iterations for the
<code>random_forest_variable_hunting</code> method. The default is <span class="math inline">\(50\)</span>.</td>
</tr>
</tbody>
</table>
<p>The unoptimised methods do not require hyperparameter optimisation,
and use default values from the <code>ranger</code> and
<code>randomForestSRC</code>. For
<code>random_forest_variable_hunting_default</code>the
<code>fs_vh_fold</code>, <code>fs_vh_step_size</code> and
<code>fs_vh_n_rep</code> parameters can be set.</p>
<div id="permutation-importance" class="section level3">
<h3>Permutation importance</h3>
<p>The permutation importance method is implemented by
<code>random_forest_permutation</code> and
<code>random_forest_permutation_default</code>
(<code>randomForestSRC</code> package) and
<code>random_forest_ranger_permutation</code> and
<code>random_forest_ranger_permutation_default</code>
(<code>ranger</code> package). In short, this method functions as
follows [Ishwaran2007-va]. As usual, each tree in the random forest is
constructed using the in-bag samples of a bootstrap of the data. The
predictive performance of each model is first measured using the
out-of-bag data. Subsequently, the out-of-bag instances for each feature
are randomly permuted, and predictive performance is assessed again. The
difference between the normal performance and the permuted performance
is used as a measure of the variable importance. For important features,
this difference is large, whereas for irrelevant features the difference
is negligible or even negative.</p>
</div>
<div id="holdout-permutation-importance" class="section level3">
<h3>Holdout permutation importance</h3>
<p>This variant on permutation importance
(<code>random_forest_ranger_holdout_permutation</code> and
<code>random_forest_ranger_holdout_permutation_default</code>) is
implemented using <code>ranger::holdoutRF</code>. Instead of using
out-of-bag to compute feature importance, two cross-validation folds are
used. A random forest is trained on either fold, and variable importance
determined on the other <span class="citation">(Janitza, Celik, and
Boulesteix 2018)</span>.</p>
<p>The hold-out variable importance method implemented in the
<code>randomForestSRC</code> package (<code>random_forest_holdout</code>
and <code>random_forest_holdout_default</code>) is implemented using
<code>randomForestSRC::holdout.vimp</code>. It is similar to the
previous variant, but does not cross-validation folds. Instead,
out-of-bag prediction errors for models trained with and without each
feature are compared.</p>
</div>
<div id="minimum-depth-variable-selection" class="section level3">
<h3>Minimum depth variable selection</h3>
<p>Important features tend to appear closer to the root of trees in
random forests. Therefore, the position of each feature within a tree is
assessed in minimum depth variable selection <span class="citation">(Ishwaran et al. 2010)</span>.</p>
</div>
<div id="variable-hunting" class="section level3">
<h3>Variable hunting</h3>
<p>Variable hunting is implemented using the variable hunting algorithm
implemented in <code>randomForestSRC</code>. Ishwaran suggest using it
when minimum depth variable selection leads to high computational load,
or a larger set of variables should be found <span class="citation">(Ishwaran et al. 2010)</span>.</p>
<p>The variable hunting selection method has several parameters which
can be set.</p>
</div>
<div id="impurity-importance" class="section level3">
<h3>Impurity importance</h3>
<p>At each node, the data is split into (two) subsets, which connects to
two branches. After splitting, each single subset is purer than the
parent dataset. As a concrete example, in regression problems the
variance of each of the subsets is lower than that of the data prior to
splitting. The decrease in variance specifically, or the decrease of
impurity generally, is then used to assess feature importance.</p>
<p><code>familiar</code> uses the <code>impurity_corrected</code>
importance measure, which is unbiased to the number of split points of a
feature and its distribution <span class="citation">(Nembrini, König,
and Wright 2018)</span>.</p>
</div>
</div>
<div id="special-methods" class="section level2">
<h2>Special methods</h2>
<p>Familiar offers several methods that are special in that they are not
feature selection methods in the sense that they determine a variable
importance that can be used for establishing feature rankings.</p>
<div id="no-feature-selection" class="section level3">
<h3>No feature selection</h3>
<p>As the name suggests, the <code>none</code> method avoids feature
selection altogether. All features are passed into a model. Feature
order is randomly shuffled prior to building a model to avoid influence
of the provided feature order.</p>
</div>
<div id="random-feature-selection" class="section level3">
<h3>Random feature selection</h3>
<p>The <code>random</code> method randomly draws features prior to model
building. It does not assign a random variable importance to a feature.
New features are drawn each time a model is built. All features are
available for the draw, but only <span class="math inline">\(m\)</span>
features are drawn. Here <span class="math inline">\(m\)</span> is the
signature size that is usually optimised by hyperparameter
optimisation.</p>
</div>
<div id="signature-only" class="section level3">
<h3>Signature only</h3>
<p>When configuring familiar, any number of features can be set as a
model signature using the <code>signature</code> configuration
parameter. However, more features may be added to this signature through
feature selection. To make sure that only the provided features enter a
model, the <code>signature_only</code> method may be used.</p>
</div>
</div>
</div>
<div id="aggregating-variable-importance" class="section level1">
<h1>Aggregating variable importance</h1>
<p>In case of feature selection or modelling in the presence of
resampling (e.g. bootstraps), the ranks of features may need to be
aggregated across the different instances <span class="citation">(Wald
et al. 2012)</span>. The rank aggregation methods shown in the table
below can be used for this purpose. Several methods require a threshold
to indicate the size of the set of most highly ranked features, which
can be set by specifying the
<code>vimp_aggregation_rank_threshold</code> configuration
parameter.</p>
<table>
<colgroup>
<col width="41%" />
<col width="25%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>aggregation method</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">none</td>
<td align="left"><code>none</code></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">mean rank</td>
<td align="left"><code>mean</code></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">median rank</td>
<td align="left"><code>median</code></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">best rank</td>
<td align="left"><code>best</code></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">worst rank</td>
<td align="left"><code>worst</code></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">stability selection</td>
<td align="left"><code>stability</code></td>
<td align="center">uses threshold</td>
</tr>
<tr class="odd">
<td align="left">exponential selection</td>
<td align="left"><code>exponential</code></td>
<td align="center">uses threshold</td>
</tr>
<tr class="even">
<td align="left">borda ranking</td>
<td align="left"><code>borda</code></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">enhanced borda ranking</td>
<td align="left"><code>enhanced_borda</code></td>
<td align="center">uses threshold</td>
</tr>
<tr class="even">
<td align="left">truncated borda ranking</td>
<td align="left"><code>truncated_borda</code></td>
<td align="center">uses threshold</td>
</tr>
<tr class="odd">
<td align="left">enhanced truncated borda ranking</td>
<td align="left"><code>enhanced_truncated_borda</code></td>
<td align="center">uses threshold</td>
</tr>
</tbody>
</table>
<div id="notation" class="section level2">
<h2>Notation</h2>
<p>Let <span class="math inline">\(N\)</span> be the number of ranking
experiments that should be aggregated. Feature <span class="math inline">\(i\)</span> for experiment <span class="math inline">\(j\)</span> of <span class="math inline">\(N\)</span> then has rank <span class="math inline">\(r_{ij}\)</span>. A lower rank indicates a more
important feature. Some features may not receive a score during a
ranking experiment, for example for multivariate variable importance
methods such as lasso regression, or by use of a threshold <span class="math inline">\(\tau\)</span>. This is designated by <span class="math inline">\(\delta_{ij}\)</span>, which is <span class="math inline">\(0\)</span> if the feature is absent, and <span class="math inline">\(1\)</span> if it is present.</p>
<p>In case a threshold is used, <span class="math inline">\(\delta_{ij}
= 1\)</span> if <span class="math inline">\(r_{ij} \leq \tau\)</span>,
and <span class="math inline">\(0\)</span> otherwise.</p>
<p>Thus, for each experiment <span class="math inline">\(m_j =
\sum^M_{i=1} \delta_{ij}\)</span> features are ranked, out of <span class="math inline">\(M\)</span> features. <span class="math inline">\(m_j\)</span> is then also the maximum rank found
in experiment <span class="math inline">\(j\)</span>.</p>
<p>Aggregating ranks for each feature results in an aggregate rank score
<span class="math inline">\(s_i\)</span>. Features are subsequently
ranked according to this method-specific score to arrive at an aggregate
feature rank <span class="math inline">\(r_i\)</span>.</p>
</div>
<div id="no-rank-aggregation" class="section level2">
<h2>No rank aggregation</h2>
<p>The <code>none</code> option does not aggregate ranks. Rather, scores
are aggregated by computing the average score of a feature over all
experiments that contain it. Ranks are then computed from the aggregated
scores.</p>
</div>
<div id="mean-rank-aggregation" class="section level2">
<h2>Mean rank aggregation</h2>
<p>The mean rank aggregation method ranks features by computing the mean
rank of a feature across all experiments that contain it.</p>
<p><span class="math display">\[s_i = \frac{\sum^{N}_{j=1} \delta_{ij}
r_{ij}}{\sum^{N}_{j=1} \delta_{ij}}\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in ascending
order.</p>
</div>
<div id="median-rank-aggregation" class="section level2">
<h2>Median rank aggregation</h2>
<p>The median rank aggregation method ranks features by computing the
median rank of a feature across all experiments that contain it.</p>
<p><span class="math display">\[s_i = \underset{j \in N, \,
\delta_{ij}=1}{\textrm{median}}(r_{ij})\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in ascending
order.</p>
</div>
<div id="best-rank-aggregation" class="section level2">
<h2>Best rank aggregation</h2>
<p>The best rank aggregation method ranks features by the best rank that
a feature has across all experiments that contain it.</p>
<p><span class="math display">\[s_i =  \underset{j \in N, \,
\delta_{ij}=1}{\textrm{min}} (r_{ij})\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in ascending
order.</p>
</div>
<div id="worst-rank-aggregation" class="section level2">
<h2>Worst rank aggregation</h2>
<p>The worst rank aggregation method ranks features by the worst rank
that a feature has across all instances that contain it.</p>
<p><span class="math display">\[s_i =  \underset{j \in N, \,
\delta_{ij}=1}{\textrm{max}} (r_{ij})\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in ascending
order.</p>
</div>
<div id="stability-rank-aggregation" class="section level2">
<h2>Stability rank aggregation</h2>
<p>The stability aggregation method ranks features by their occurrence
within the set of highly ranked features across all experiments. Our
implementation generalises the method originally proposed by Meinshausen
and Bühlmann <span class="citation">(Meinshausen and Bühlmann
2010)</span>.</p>
<p>This method uses threshold <span class="math inline">\(\tau\)</span>
to designate the highly ranked features. Thus <span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(r_{ij} \leq \tau\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<p>The aggregate rank score is computed as:</p>
<p><span class="math display">\[s_i = \frac{1}{N} \sum^N_{j=1}
\delta_{ij}\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order, as more commonly occurring features are considered more
important.</p>
</div>
<div id="exponential-rank-aggregation" class="section level2">
<h2>Exponential rank aggregation</h2>
<p>The exponential aggregation method ranks features by the sum of the
negative exponentials of their normalised ranks in instances where they
occur within the set of highly ranked features. This method was
originally suggested by Haury et al. <span class="citation">(Haury,
Gestraud, and Vert 2011)</span>.</p>
<p>This method uses threshold <span class="math inline">\(\tau\)</span>
to designate the highly ranked features. Thus <span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(r_{ij} \leq \tau\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<p><span class="math display">\[s_i = \sum^N_{j=1} \delta_{ij}
\exp({-r_{ij} / \tau)}\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order.</p>
</div>
<div id="borda-rank-aggregation" class="section level2">
<h2>Borda rank aggregation</h2>
<p>Borda rank aggregation ranks a feature by the sum of normalised ranks
(the borda score) across all experiments that contain it. In case every
experiment contains all features, the result is equivalent to the mean
aggregation method <span class="citation">(Wald et al. 2012)</span>.</p>
<p><span class="math display">\[s_i = \sum^N_{j=1} \frac{m_j - r_{ij} +
1}{m_j}\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order.</p>
</div>
<div id="enhanced-borda-rank-aggregation" class="section level2">
<h2>Enhanced borda rank aggregation</h2>
<p>Enhanced borda rank aggregation combines borda rank aggregation with
stability rank aggregation. The borda score is multiplied by the
occurrence of the feature within the set of highly ranked features
across all experiments <span class="citation">(Wald et al.
2012)</span>.</p>
<p>This method uses threshold <span class="math inline">\(\tau\)</span>
to designate the highly ranked features for the purpose of computing the
occurrence. Thus <span class="math inline">\(\delta_{ij} = 1\)</span> if
<span class="math inline">\(r_{ij} \leq \tau\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<p><span class="math display">\[s_i = \left( \frac{1}{N} \sum^N_{j=1}
\delta_{ij} \right) \left( \sum^N_{j=1}
\frac{m_j - r_{ij} + 1}{m_j} \right)\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order.</p>
</div>
<div id="truncated-borda-rank-aggregation" class="section level2">
<h2>Truncated borda rank aggregation</h2>
<p>Truncated borda rank aggregation is borda rank aggregation performed
with only the set of most highly ranked features in each instance.</p>
<p>This method uses threshold <span class="math inline">\(\tau\)</span>
to designate the highly ranked features. Thus <span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(r_{ij} \leq \tau\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<p><span class="math display">\[s_i = \sum^N_{j=1} \delta_{ij}
\frac{\tau - r_{ij} + 1}{\tau}\]</span></p>
<p>Note that compared to the borda method, the number of ranked features
in an experiment <span class="math inline">\(m_j\)</span> is replaced by
threshold <span class="math inline">\(\tau\)</span>.</p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order.</p>
</div>
<div id="truncated-enhanced-borda-rank-aggregation" class="section level2">
<h2>Truncated enhanced borda rank aggregation</h2>
<p>Truncated enhanced borda rank aggregation is enhanced borda
aggregation performed with only the set of most highly ranked features
in each experiment.</p>
<p>This method uses threshold <span class="math inline">\(\tau\)</span>
to designate the highly ranked features. Thus <span class="math inline">\(\delta_{ij} = 1\)</span> if <span class="math inline">\(r_{ij} \leq \tau\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<p><span class="math display">\[s_i = \left( \frac{1}{N} \sum^N_{j=1}
\delta_{ij} \right) \left( \sum^N_{j=1}
\delta_{ij} \frac{\tau - r_{ij} + 1}{\tau} \right)\]</span></p>
<p>The aggregate rank of features is then determined by sorting
aggregate scores <span class="math inline">\(s_i\)</span> in descending
order.</p>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Battiti1994-ja" class="csl-entry">
Battiti, R. 1994. <span>“Using Mutual Information for Selecting Features
in Supervised Neural Net Learning.”</span> <em>IEEE Trans. Neural
Netw.</em> 5 (4): 537–50.
</div>
<div id="ref-De_Jay2013-yl" class="csl-entry">
De Jay, Nicolas, Simon Papillon-Cavanagh, Catharina Olsen, Nehme
El-Hachem, Gianluca Bontempi, and Benjamin Haibe-Kains. 2013.
<span>“<span class="nocase">mRMRe</span>: An <span>R</span> Package for
Parallelized <span class="nocase">mRMR</span> Ensemble Feature
Selection.”</span> <em>Bioinformatics</em> 29 (18): 2365–68.
</div>
<div id="ref-Gelfand1959-de" class="csl-entry">
Gel′fand, I M, and A M Yaglom. 1959. <span>“Calculation of the Amount of
Information about a Random Function Contained in Another Such
Function.”</span> In <em>Eleven Papers on Analysis, Probability and
Topology</em>, edited by E B Dynkin, I M Gel’fand, A O Gel’fond, and M A
Krasnosel’skii, 12:199–246. American Mathematical Society Translations:
Series 2. Providence, Rhode Island: American Mathematical Society.
</div>
<div id="ref-Hastie2009-ed" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction</em>. Second Edition. Springer Series in Statistics. New
York, NY, United States: Springer Science+Business Media, LLC.
</div>
<div id="ref-Haury2011-zd" class="csl-entry">
Haury, Anne-Claire, Pierre Gestraud, and Jean-Philippe Vert. 2011.
<span>“The Influence of Feature Selection Methods on Accuracy, Stability
and Interpretability of Molecular Signatures.”</span> <em>PLoS One</em>
6 (12): e28210.
</div>
<div id="ref-Ishwaran2010-zv" class="csl-entry">
Ishwaran, Hemant, Udaya B Kogalur, Eiran Z Gorodeski, Andy J Minn, and
Michael S Lauer. 2010. <span>“<span>High-Dimensional</span> Variable
Selection for Survival Data.”</span> <em>J. Am. Stat. Assoc.</em> 105
(489): 205–17.
</div>
<div id="ref-Janitza2018-kl" class="csl-entry">
Janitza, Silke, Ender Celik, and Anne-Laure Boulesteix. 2018. <span>“A
Computationally Fast Variable Importance Test for Random Forests for
High-Dimensional Data.”</span> <em>Adv. Data Anal. Classif.</em> 12 (4):
885–915.
</div>
<div id="ref-Meinshausen2010-do" class="csl-entry">
Meinshausen, Nicolai, and Peter Bühlmann. 2010. <span>“Stability
Selection.”</span> <em>J. R. Stat. Soc. Series B Stat. Methodol.</em> 72
(4): 417–73.
</div>
<div id="ref-Nembrini2018-ay" class="csl-entry">
Nembrini, Stefano, Inke R König, and Marvin N Wright. 2018. <span>“The
Revival of the Gini Importance?”</span> <em>Bioinformatics</em> 34 (21):
3711–18.
</div>
<div id="ref-Oshiro2012-mq" class="csl-entry">
Oshiro, Thais Mayumi, Pedro Santoro Perez, and José Augusto Baranauskas.
2012. <span>“How Many Trees in a Random Forest?”</span> In <em>Machine
Learning and Data Mining in Pattern Recognition</em>, 154–68. Springer
Berlin Heidelberg.
</div>
<div id="ref-Peng2005-oo" class="csl-entry">
Peng, Hanchuan, Fuhui Long, and Chris Ding. 2005. <span>“Feature
Selection Based on Mutual Information: Criteria of Max-Dependency,
Max-Relevance, and Min-Redundancy.”</span> <em>IEEE Trans. Pattern Anal.
Mach. Intell.</em> 27 (8): 1226–38.
</div>
<div id="ref-rcore2018" class="csl-entry">
R Core Team. 2019. <em>R: A Language and Environment for Statistical
Computing</em>. Vienna, Austria: R Foundation for Statistical Computing.
<a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-Simon2011-ih" class="csl-entry">
Simon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011.
<span>“Regularization Paths for Cox’s Proportional Hazards Model via
Coordinate Descent.”</span> <em>J. Stat. Softw.</em> 39 (5): 1–13.
</div>
<div id="ref-Wald2012-zk" class="csl-entry">
Wald, R, T M Khoshgoftaar, D Dittman, W Awada, and A Napolitano. 2012.
<span>“An Extensive Comparison of Feature Ranking Aggregation Techniques
in Bioinformatics.”</span> In <em>2012 <span>IEEE</span> 13th
International Conference on Information Reuse Integration
(<span>IRI</span>)</em>, 377–84.
</div>
</div>
</div>

<div class="footer">
<br>
<a rel="license" href="https://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFgAAAAfCAMAAABUFvrSAAAAIGNIUk0AAHolAACAgwAA+f8AAIDpAAB1MAAA6mAAADqYAAAXb5JfxUYAAAAEZ0FNQQAAsY58+1GTAAAAAXNSR0IB2cksfwAAAW5QTFRF////////////////7+/v39/f1tXV09bS0tXS0tXR0dTR0dTQ0NTQ0NPPz9PPztLOztHNzdHNzdHMz8/PzdDMzNDMzNDLzM/Ly8/Ly8/Ky87Kys3Jyc3Jyc3IyMzIyMzHx8vHxsrGxsrFxcnFxcnExMnExMjDw8jDxMfDw8fCwsfCwcXAwMXAwMW/wMS/v8S+v8O+vsO+vsK9vcK9vcK8v7+/vMG8vMG7vMC8u8C7u8C6ur+6ur+5ub65ub64uL23t7y2tru1tbq0tLqztLmzs7iysrixsrexsbewsbawsLavsLWvr7Wur7SusLOvrrStrrOtr7KvrbOsrLKrr6+vq7Gqn6OenqCdn5+flpmWk5iTkZSRkZORj4+PiYyJhIaEhIWEgoWCgICAfX98fH98eXx5cHJvcHBwYGBgXV5dUFFQUFBQQ0RDQEBAPj8+NTY1MjMxMDAwKSkpKCkoICAgGxsbEBAQDg4ODQ4NAAAAlzoSDQAAAAN0Uk5TAAoO5yEBUwAAAvhJREFUeNq1lutX2kAQxWmXFDVGYy1EIjQ2VZDiu1CsRQQURYvV+qSKj6II8rANYOT+9z0JqIASo9Y5ydkP2f2d2Ts7d2N4jRcJgwEIBwO+SbdTFGw8ZzZz1n5BdLgnfLPBcCT6fW1jY3P78QEYEA76PWMu0W5lGbrNZGrrYNg+u+ga9fgVcmxtY/NJZAOCfs+IY4Bn6eN8RdlEJX9Ed1uFIfdnfzC8uBJbv5tyqqhMLKa0wQHPiEOwMInLW4Eu9xmzfdDtmQ0uLK3cSXmvBBTS6QJQ2tMC+8YcgpnOApAzSa83mZEBZIff2odGfYFQJNqc8s4VchQhhFA5XO1pgCddAxaFKyeNpBpxGSgNmwXXxMxcWE25fkkJGUIIoExESQPsFnkmC0gUuQmjBGQZq+j2BEKR5dUGLVLIvbkGkxxSrcHO92wCkIyENJL3u+2O8Zng/FJsvR5cRF0GFIqtwaKVvoTcSxrCKOOS7hPdXwLhxUYtUFC+Z6AKQgpoDRZ6joEkaYo4cMQKril/KLLcCE4TVYmqFmkNsK0rD9lIiDdXKCSrwwEhREae6Ve0WIiuPg3M0xVlW171BBe21CGjbLbSYR0c/To3H409TQquHTggREKZ8pbjEiRqqxxXtWjjRLdvLrzUAK4Vr5qwZvEsJsCrzExWF9Tk9gIm84e74BRyRN9xeyS4vkHSmg1yK4Wxt5yUIClDayn0t3SteLWq3RQvjQrN31O87e2dEiBl0tJDJmTrykImN8dtq6AOpIw8Y3OMf2s+bvptU+hJqFrc1yCfpmZDkWYX0mv0H9WWpvS2tH6w8z27e58JJVi7c2ImuNBkQvrBOOWZc0CqsyFKtU3+97OuaQBnXGe90RuTMvCHtpziuWCcmDvPm64m+t2vlmuq/YHqqwnGCcfs1l+mCcbSmgtSe8iDGQNnPEsnrq//fZrltXS4tk3oAOPvT2tPF91uMrXTDNv340JrjQ4hbsHAxeE0z1ksHD99eKFdl0dl/P//Cl+9EPcfS+yBAoqk3eUAAAAASUVORK5CYII=" /></a>
This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Cite as: Alex Zwanenburg. familiar: Vignettes and Documentation (2021). <a href="https://github.com/alexzwanenburg/familiar">https://github.com/alexzwanenburg/familiar</a>
</div>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
