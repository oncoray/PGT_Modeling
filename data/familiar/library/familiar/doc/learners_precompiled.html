<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Alex Zwanenburg" />

<meta name="date" content="2022-12-16" />

<title>Learning algorithms and hyperparameter optimisation</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<img src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   version="1.1"
   id="Layer_1"
   x="0px"
   y="0px"
   viewBox="0 0 735 852"
   style="enable-background:new 0 0 735 852;"
   xml:space="preserve"
   sodipodi:docname="familiar.svg"
   inkscape:version="1.0.1 (3bc2e813f5, 2020-09-07)"><metadata
   id="metadata79"><rdf:RDF><cc:Work
       rdf:about=""><dc:format>image/svg+xml</dc:format><dc:type
         rdf:resource="http://purl.org/dc/dcmitype/StillImage" /></cc:Work></rdf:RDF></metadata><defs
   id="defs77"><clipPath
     clipPathUnits="userSpaceOnUse"
     id="clipPath1261"><path
       class="st0"
       d="M 18.2,626.4 V 223.1 L 69.8,193.3 367.5,21.5 v 0 l 296.7,171.3 52.6,30.3 V 626.4 L 367.5,828 Z"
       id="path1263"
       style="display:inline;fill:#ffd5e5" /></clipPath></defs><sodipodi:namedview
   pagecolor="#ffffff"
   bordercolor="#666666"
   borderopacity="1"
   objecttolerance="10"
   gridtolerance="10"
   guidetolerance="10"
   inkscape:pageopacity="0"
   inkscape:pageshadow="2"
   inkscape:window-width="2489"
   inkscape:window-height="1289"
   id="namedview75"
   showgrid="false"
   inkscape:zoom="0.70539906"
   inkscape:cx="455.24253"
   inkscape:cy="60.680079"
   inkscape:window-x="0"
   inkscape:window-y="0"
   inkscape:window-maximized="0"
   inkscape:current-layer="layer2" />
<style
   type="text/css"
   id="style2">
	.st0{fill:#144F85;}
	.st1{fill:#173E6C;}
	.st2{fill:#E6B35A;}
	.st3{fill:#FFFFFF;}
	.st4{fill:#231F20;}
</style>
<g
   inkscape:groupmode="layer"
   id="layer3"
   inkscape:label="Hexagon"
   style="display:inline"><path
     class="st0"
     d="M 18.2,626.4 V 223.1 L 69.8,193.3 367.5,21.5 v 0 l 296.7,171.3 52.6,30.3 V 626.4 L 367.5,828 Z"
     id="path4"
     style="fill:#ffd5e5" /><path
     class="st1"
     d="M 705.6,196.2 427.8,35.8 367.5,1 307.2,35.8 29.4,196.2 0.1,213.1 v 424.2 l 29.3,16.9 281.3,162.4 56.9,32.8 56.9,-32.8 281.3,-162.4 29.3,-16.9 V 213.1 Z m 0,425.3 L 367.5,816.7 29.4,621.5 V 231 L 367.5,35.8 v 0 L 705.6,231 Z"
     id="path8"
     style="fill:#ffaacc" /></g><g
   inkscape:groupmode="layer"
   id="layer2"
   inkscape:label="NetworkBG"><g
     id="g1203"
     clip-path="url(#clipPath1261)"><path
       style="fill:#ffeeaa;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 604.82727,123.80744 368.81604,260.06859 298.81945,261.12628 1.0461121,433.04579 29.3512,482.21648 327.12453,310.29697 397.12112,309.23928 633.13236,172.97813 Z"
       id="path1032-8"
       sodipodi:nodetypes="ccccccccc" /><path
       style="fill:#ffeeaa;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 524.66016,-15.335647 288.64893,120.9255 218.65234,121.98319 -79.120987,293.9027 -50.815883,343.07338 246.95744,171.15387 316.95403,170.09618 552.96527,33.835032 Z"
       id="path1032-8-8"
       sodipodi:nodetypes="ccccccccc" /><path
       style="fill:#ffdd55;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
       d="M 552.96527,33.835032 316.95403,170.09618 246.95744,171.15387 -50.815883,343.07338 1.0461121,433.04579 298.81944,261.12628 368.81603,260.06859 604.82726,123.80744 Z"
       id="path1032-8-8-7"
       sodipodi:nodetypes="ccccccccc" /></g><path
     class="st1"
     d="M 705.6,196.2 427.8,35.8 367.5,1 307.2,35.8 29.4,196.2 0.1,213.1 v 424.2 l 29.3,16.9 281.3,162.4 56.9,32.8 56.9,-32.8 281.3,-162.4 29.3,-16.9 V 213.1 Z m 0,425.3 L 367.5,816.7 29.4,621.5 V 231 L 367.5,35.8 v 0 L 705.6,231 Z"
     id="path8-4"
     style="display:inline;fill:#ffaacc" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0"
     sodipodi:sides="5"
     sodipodi:cx="593.43592"
     sodipodi:cy="273.58523"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 628.55341,299.12523 -28.41819,-4.88808 -20.13738,20.6391 -4.13287,-28.5378 -25.85174,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43055,25.5168 z"
     inkscape:transform-center-x="4.1403908"
     inkscape:transform-center-y="-0.0063989586" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0-5"
     sodipodi:sides="5"
     sodipodi:cx="659.08881"
     sodipodi:cy="357.25101"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 694.2063,382.791 -28.41819,-4.88808 -20.13738,20.6391 -4.13286,-28.5378 -25.85175,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43054,25.5168 z"
     inkscape:transform-center-x="-2.3762241"
     inkscape:transform-center-y="-0.19639333"
     transform="matrix(0.50883698,0.35123436,-0.35123436,0.50883698,437.9071,-51.238505)" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:#ffeeaa;stroke-width:10;stroke-linecap:round;stroke-linejoin:round;stroke-miterlimit:4;stroke-dasharray:none"
     id="path1013-0-5-4"
     sodipodi:sides="5"
     sodipodi:cx="659.08881"
     sodipodi:cy="357.25101"
     sodipodi:r1="43.422688"
     sodipodi:r2="21.711344"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 694.2063,382.791 -28.41819,-4.88808 -20.13738,20.6391 -4.13286,-28.5378 -25.85175,-12.77396 25.86394,-12.74925 4.16013,-28.53384 20.11765,20.65833 28.42285,-4.86092 -13.43054,25.5168 z"
     inkscape:transform-center-x="-2.3762241"
     inkscape:transform-center-y="-0.19639333"
     transform="matrix(0.50883698,0.35123436,-0.35123436,0.50883698,458.27634,-128.13567)" /></g><g
   inkscape:groupmode="layer"
   id="layer1"
   inkscape:label="Familiar"
   style="display:inline"
   sodipodi:insensitive="true"><path
     id="rect1265"
     style="fill:#ffd5e5;stroke:none;stroke-width:0.888669;stroke-linecap:round;stroke-linejoin:round"
     d="m 245.46055,257.17563 229.95737,-15.59401 25.54911,67.75725 -0.0316,100.16657 -113.21601,42.22457 -148.63819,-42.93339 z"
     sodipodi:nodetypes="ccccccc" /><path
     style="fill:#d40055;stroke:none;stroke-width:3.16295"
     d="m 196.18676,315.6231 c -10.63253,17.38577 -33.64223,11.64186 -43.00737,28.79103 -36.61508,67.04939 17.43539,145.00334 62.13877,186.97879 17.5006,16.4315 42.7508,54.66573 50.734,46.89383 7.9832,-7.7719 -15.8147,-44.28123 -15.8147,-44.28123 28.2966,-20.2857 82.299,43.82139 86.6834,19.30534 4.3845,-24.51606 -46.8635,-18.95742 -67.7058,-44.60891 18.3594,0.003 59.6957,1.2462 63.2589,-9.48884 3.5633,-10.73504 -72.7477,-12.65178 -72.7477,-12.65178 v -9.48883 c 34.0706,-9.2959 57.6399,0.0886 91.7254,-0.81921 40.6388,-1.08172 68.0795,-15.63444 104.3772,7.1451 -25.2055,6.06969 -72.3313,25.48918 -69.5848,31.62945 2.7465,6.14027 72.7477,-3.16294 72.7477,-3.16294 0,0 -57.9896,56.7778 -37.9553,63.25891 20.0343,6.48111 47.0025,-53.26957 69.2495,-46.34981 18.3641,5.25049 12.4794,48.94683 25.6389,46.34981 13.1595,-2.59702 -1.3633,-35.41234 4.7665,-53.77008 15.9318,-47.71619 71.4573,-124.46633 44.8348,-177.06643 -9.8589,-19.48185 -35.2004,-18.55479 -45.9956,-38.02841 -10.0929,-18.20718 -8.0181,-37.2576 -22.925,-53.74319 -34.35974,-24.66838 -39.39345,-40.50377 -107.1986,-44.29358 0,0 -79.32687,1.91436 -115.39694,20.37701 -36.07006,18.46265 -63.11016,62.96566 -77.82326,87.02397 z m 288.1086,-44.14206 c -31.1126,3.20944 -51.2432,56.52436 -12.6391,55.74438 9.2389,-0.18661 17.684,-6.95247 25.2909,-11.46315 14.9228,44.08514 -20.2808,63.4528 -60.096,71.8422 -66.9715,14.1118 -129.6121,21.60703 -189.7767,-18.07212 4.9516,-26.65573 -6.3679,-90.45044 15.8758,-103.05699 11.5245,-6.53148 25.0878,-7.94627 37.6087,-3.39226 16.6016,6.03838 24.9053,24.81679 41.4064,32.38129 36.5393,16.75032 91.7428,-5.26188 88.5599,-49.28692 22.9719,0 49.3293,-3.21766 53.7701,25.30357 m -205.2498,18.1746 c -31.0111,12.78146 -7.1581,66.184 24.286,52.66209 29.17,-12.54392 6.5498,-65.37112 -24.286,-52.66209 m 132.5021,41.92136 0.8771,11.7139 8.3914,-0.0645 -1.0516,-13.03381 -8.2169,1.38445 m -50.6071,3.16295 3.1629,12.65178 h 6.3259 l 3.1629,-12.65178 h -12.6517 m 142.3325,50.60712 c -51.7831,62.49348 -152.6773,51.3441 -221.4062,34.79241 0,0 79.3233,3.56324 116.6478,-2.66733 37.3245,-6.23056 104.7584,-32.12508 104.7584,-32.12508 m -53.7701,75.9107 c 7.8662,-5.63637 54.0041,-39.90056 58.1855,-37.39867 10.656,6.37966 -2.9763,25.22449 -8.2552,30.35163 -13.2433,12.85737 -33.4608,11.91481 -49.9303,7.04704 m -192.9396,-6.32589 v 6.32589 c -22.296,10.71289 -35.7552,2.09071 -37.9554,-22.14062 z"
     id="path988"
     sodipodi:nodetypes="scczczczcccczczczccscczscscccssscccsccccccccccccczccccccccc" /><text
     xml:space="preserve"
     style="font-size:106.667px;line-height:1.25;font-family:Bahnschrift;-inkscape-font-specification:Bahnschrift;fill:#ffffff"
     x="133.01973"
     y="669.02026"
     id="text1007"><tspan
       sodipodi:role="line"
       id="tspan1005"
       x="133.01973"
       y="669.02026"
       style="font-style:normal;font-variant:normal;font-weight:normal;font-stretch:normal;font-size:106.667px;font-family:'Lucida Sans';-inkscape-font-specification:'Lucida Sans';fill:#ffffff">FAMILIAR</tspan></text><path
     style="fill:#ffaacc;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1"
     d="M 362.57822,85.512104 311.7313,246.86829 c 0,0 25.2203,31.68457 54.88255,30.82445 36.78744,-1.06673 54.26709,-35.12035 54.26709,-35.12035 z"
     id="path1009"
     sodipodi:nodetypes="ccscc" /><path
     sodipodi:type="star"
     style="fill:#ffe680;stroke:none;stroke-width:0.463344;stroke-linecap:round;stroke-linejoin:round"
     id="path1011"
     sodipodi:sides="5"
     sodipodi:cx="393.43726"
     sodipodi:cy="240.33679"
     sodipodi:r1="21.630606"
     sodipodi:r2="10.815303"
     sodipodi:arg1="1.300471"
     sodipodi:arg2="1.9287895"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 399.2136,261.18186 -9.56598,-10.71543 -14.25022,1.80547 7.23494,-12.40903 -6.12067,-12.99484 14.03741,3.04623 10.46744,-9.83673 1.44066,14.29171 12.5899,6.9154 -13.14703,5.78653 z"
     inkscape:transform-center-x="-1.7849856"
     inkscape:transform-center-y="0.29290531" /><path
     sodipodi:type="star"
     style="fill:#ffe680;stroke:none;stroke-width:0.552007;stroke-linecap:round;stroke-linejoin:round"
     id="path1013"
     sodipodi:sides="5"
     sodipodi:cx="365.95331"
     sodipodi:cy="171.80791"
     sodipodi:r1="24.417637"
     sodipodi:r2="12.208818"
     sodipodi:arg1="0.62879629"
     sodipodi:arg2="1.2571148"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 385.70073,186.16967 -15.98024,-2.74869 -11.32374,11.60587 -2.32401,-16.0475 -14.53707,-7.18311 14.54393,-7.16921 2.33934,-16.04528 11.31265,11.61668 15.98286,-2.73341 -7.55232,14.34872 z"
     inkscape:transform-center-x="2.3282526"
     inkscape:transform-center-y="-0.0036080646" /><path
     sodipodi:type="star"
     style="display:inline;fill:#ffe680;stroke:none;stroke-width:0.440145;stroke-linecap:round;stroke-linejoin:round"
     id="path1011-6"
     sodipodi:sides="5"
     sodipodi:cx="344.4216"
     sodipodi:cy="225.49474"
     sodipodi:r1="20.547558"
     sodipodi:r2="10.273779"
     sodipodi:arg1="1.300471"
     sodipodi:arg2="1.9287895"
     inkscape:flatsided="false"
     inkscape:rounded="0"
     inkscape:randomized="0"
     d="m 349.90872,245.29609 -9.087,-10.17891 -13.53671,1.71507 6.87268,-11.78771 -5.81421,-12.34419 13.33456,2.89371 9.94333,-9.34421 1.36853,13.57613 11.95952,6.56915 -12.48876,5.4968 z"
     inkscape:transform-center-x="-1.6956102"
     inkscape:transform-center-y="0.27822403" /></g>
<title
   id="title6">feather</title>

































</svg>
" align="right" width="120" />

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>






<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Learning algorithms and hyperparameter
optimisation</h1>
<h4 class="author">Alex Zwanenburg</h4>
<h4 class="date">2022-12-16</h4>


<div id="TOC">
<ul>
<li><a href="#configuration-options" id="toc-configuration-options">Configuration options</a></li>
<li><a href="#generalised-linear-models" id="toc-generalised-linear-models">Generalised linear models</a>
<ul>
<li><a href="#linear-models-for-binomial-outcomes" id="toc-linear-models-for-binomial-outcomes">Linear models for binomial
outcomes</a></li>
<li><a href="#linear-models-for-multinomial-outcomes" id="toc-linear-models-for-multinomial-outcomes">Linear models for
multinomial outcomes</a></li>
<li><a href="#linear-models-for-continuous-and-count-type-outcomes" id="toc-linear-models-for-continuous-and-count-type-outcomes">Linear
models for continuous and count-type outcomes</a></li>
<li><a href="#linear-models-for-survival-outcomes" id="toc-linear-models-for-survival-outcomes">Linear models for survival
outcomes</a></li>
</ul></li>
<li><a href="#lasso-ridge-and-elastic-net-regression" id="toc-lasso-ridge-and-elastic-net-regression">Lasso, ridge and elastic
net regression</a></li>
<li><a href="#boosted-generalised-linear-models-and-regression-trees" id="toc-boosted-generalised-linear-models-and-regression-trees">Boosted
generalised linear models and regression trees</a></li>
<li><a href="#extreme-gradient-boosted-linear-models-and-trees" id="toc-extreme-gradient-boosted-linear-models-and-trees">Extreme
gradient boosted linear models and trees</a></li>
<li><a href="#random-forest-rfsrc" id="toc-random-forest-rfsrc">Random
forest (RFSRC)</a></li>
<li><a href="#random-forest-ranger" id="toc-random-forest-ranger">Random
forest (ranger)</a></li>
<li><a href="#naive-bayes" id="toc-naive-bayes">Naive Bayes</a></li>
<li><a href="#k-nearest-neighbours" id="toc-k-nearest-neighbours"><em>k</em>-nearest neighbours</a></li>
<li><a href="#support-vector-machines" id="toc-support-vector-machines">Support vector machines</a></li>
<li><a href="#hyperparameter-optimization" id="toc-hyperparameter-optimization">Hyperparameter optimization</a>
<ul>
<li><a href="#predicting-run-time-of-model" id="toc-predicting-run-time-of-model">Predicting run time of
model</a></li>
<li><a href="#assessing-goodness-of-hyperparameter-sets" id="toc-assessing-goodness-of-hyperparameter-sets">Assessing goodness of
hyperparameter sets</a></li>
<li><a href="#predicting-optimisation-score-for-new-hyperparameter-sets" id="toc-predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a></li>
<li><a href="#acquisition-functions-for-utility-of-hyperparameter-sets" id="toc-acquisition-functions-for-utility-of-hyperparameter-sets">Acquisition
functions for utility of hyperparameter sets</a></li>
<li><a href="#exploring-challenger-sets" id="toc-exploring-challenger-sets">Exploring challenger sets</a></li>
<li><a href="#providing-hyperparameters-manually" id="toc-providing-hyperparameters-manually">Providing hyperparameters
manually</a></li>
<li><a href="#configuration-options-for-hyperparameter-optimisation" id="toc-configuration-options-for-hyperparameter-optimisation">Configuration
options for hyperparameter optimisation</a></li>
</ul></li>
<li><a href="#model-recalibration" id="toc-model-recalibration">Model
recalibration</a></li>
<li><a href="#references" id="toc-references">References</a></li>
</ul>
</div>

<p>Learning algorithms create models that relate input data to the
outcome. Development data is used to train models, after which they can
be used to predict outcomes for new data. Familiar implements several
commonly used algorithms. This vignette first describes the learners and
their hyperparameters. Then, hyperparameter optimisation is described in
more detail.</p>
<table>
<caption>Overview of learners implemented in familiar. <sup>a</sup>
These learners test multiple distributions or linking families and
attempt to find the best option. <sup>b</sup> k-nearest neighbours
learners allow for setting the distance metric using <code>*</code>. If
omitted (e.g. <code>knn</code>), the kernel is either
<code>euclidean</code> (numeric features) or <code>gower</code> (mixed
features). <sup>c</sup> The SVM kernel is indicated by <code>*</code>.
If omitted (e.g. <code>svm_c</code>), the radial basis function is used
as kernel. <sup>d</sup> SVM type is indicated by <code>*</code>, and
must be one of <code>c</code>, <code>nu</code>, or
<code>epsilon</code>.</caption>
<colgroup>
<col width="21%" />
<col width="21%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>learner</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>binomial</strong></th>
<th align="center"><strong>multinomial</strong></th>
<th align="center"><strong>continuous</strong></th>
<th align="center"><strong>count</strong></th>
<th align="center"><strong>survival</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>generalised linear models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>glm</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>glm_logistic</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cauchy</td>
<td align="left"><code>glm_cauchy</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">complementary log-log</td>
<td align="left"><code>glm_loglog</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">normal</td>
<td align="left"><code>glm_probit</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">multinomial</td>
<td align="left"><code>glm_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">log-normal</td>
<td align="left"><code>glm_log</code>,
<code>glm_log_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">normal (gaussian)</td>
<td align="left"><code>glm_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">inverse gaussian</td>
<td align="left"><code>glm_inv_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">log-poisson</td>
<td align="left"><code>glm_log_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">x</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>glm_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">x</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">exponential</td>
<td align="left"><code>survival_regr_exponential</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">gaussian</td>
<td align="left"><code>survival_regr_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">logistic</td>
<td align="left"><code>survival_regr_logistic</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">log-logistic</td>
<td align="left"><code>survival_regr_loglogistic</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">log-normal</td>
<td align="left"><code>survival_regr_lognormal</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">survival regression<sup>a</sup></td>
<td align="left"><code>survival_regr</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">weibull</td>
<td align="left"><code>survival_regr_weibull</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>lasso regression models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>lasso</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>lasso_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">multi-logistic</td>
<td align="left"><code>lasso_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">normal (gaussian)</td>
<td align="left"><code>lasso_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>lasso_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>lasso_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>ridge regression models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>ridge</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">logistic</td>
<td align="left"><code>ridge_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">multi-logistic</td>
<td align="left"><code>ridge_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">normal (gaussian)</td>
<td align="left"><code>ridge_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">poisson</td>
<td align="left"><code>ridge_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cox</td>
<td align="left"><code>ridge_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>elastic net regression models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>elastic_net</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>elastic_net_binomial</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">multi-logistic</td>
<td align="left"><code>elastic_net_multinomial</code></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">normal (gaussian)</td>
<td align="left"><code>elastic_net_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>elastic_net_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>elastic_net_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>boosted linear models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>boosted_glm</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">area under the curve</td>
<td align="left"><code>boosted_glm_auc</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cauchy</td>
<td align="left"><code>boosted_glm_cauchy</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">complementary log-log</td>
<td align="left"><code>boosted_glm_loglog</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>boosted_glm_logistic</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">log-logistic</td>
<td align="left"><code>boosted_glm_log</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">normal</td>
<td align="left"><code>boosted_glm_probit</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">gaussian</td>
<td align="left"><code>boosted_glm_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">huber loss</td>
<td align="left"><code>boosted_glm_huber</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">laplace</td>
<td align="left"><code>boosted_glm_laplace</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">poisson</td>
<td align="left"><code>boosted_glm_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">concordance index</td>
<td align="left"><code>boosted_glm_cindex</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>boosted_glm_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">log-logistic</td>
<td align="left"><code>boosted_glm_loglog</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">log-normal</td>
<td align="left"><code>boosted_glm_lognormal</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">rank-based estimation</td>
<td align="left"><code>boosted_glm_gehan</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">survival regression<sup>a</sup></td>
<td align="left"><code>boosted_glm_surv</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">weibull</td>
<td align="left"><code>boosted_glm_weibull</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>extreme gradient boosted linear
models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>xgboost_lm</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>xgboost_lm_logistic</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">gamma</td>
<td align="left"><code>xgboost_lm_gamma</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">gaussian</td>
<td align="left"><code>xgboost_lm_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">x</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>xgboost_lm_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>xgboost_lm_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>random forests</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">random forest (RFSRC)</td>
<td align="left"><code>random_forest_rfsrc</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">random forest (ranger)</td>
<td align="left"><code>random_forest_ranger</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>boosted regression trees</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>boosted_tree</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">area under the curve</td>
<td align="left"><code>boosted_tree_auc</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cauchy</td>
<td align="left"><code>boosted_tree_cauchy</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">complementary log-log</td>
<td align="left"><code>boosted_tree_loglog</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">logistic</td>
<td align="left"><code>boosted_tree_logistic</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">log-logistic</td>
<td align="left"><code>boosted_tree_log</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">normal</td>
<td align="left"><code>boosted_tree_probit</code></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">gaussian</td>
<td align="left"><code>boosted_tree_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">huber loss</td>
<td align="left"><code>boosted_tree_huber</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">laplace</td>
<td align="left"><code>boosted_tree_laplace</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>boosted_tree_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">concordance index</td>
<td align="left"><code>boosted_tree_cindex</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">cox</td>
<td align="left"><code>boosted_tree_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">log-logistic</td>
<td align="left"><code>boosted_tree_loglog</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">log-normal</td>
<td align="left"><code>boosted_tree_lognormal</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">rank-based estimation</td>
<td align="left"><code>boosted_tree_gehan</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">survival regression<sup>a</sup></td>
<td align="left"><code>boosted_tree_surv</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">weibull</td>
<td align="left"><code>boosted_tree_weibull</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>extreme gradient boosted trees</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>xgboost_tree</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left">logistic</td>
<td align="left"><code>xgboost_tree_logistic</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">gamma</td>
<td align="left"><code>xgboost_tree_gamma</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">gaussian</td>
<td align="left"><code>xgboost_tree_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">x</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">poisson</td>
<td align="left"><code>xgboost_tree_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">cox</td>
<td align="left"><code>xgboost_tree_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left"><strong>extreme gradient boosted DART
trees</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">general<sup>a</sup></td>
<td align="left"><code>xgboost_dart</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
</tr>
<tr class="odd">
<td align="left">logistic</td>
<td align="left"><code>xgboost_dart_logistic</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">gamma</td>
<td align="left"><code>xgboost_dart_gamma</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">gaussian</td>
<td align="left"><code>xgboost_dart_gaussian</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">x</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">poisson</td>
<td align="left"><code>xgboost_dart_poisson</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">cox</td>
<td align="left"><code>xgboost_dart_cox</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
</tr>
<tr class="even">
<td align="left"><strong>bayesian models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">naive bayes</td>
<td align="left"><code>naive_bayes</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><strong>nearest neighbour models</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">k-nearest neighbours<sup>b</sup></td>
<td align="left"><code>k_nearest_neighbours_*</code>,
<code>knn_*</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">x</td>
<td align="center">x</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><strong>support vector machines</strong></td>
<td align="left"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(C\)</span>-classification<sup>c</sup></td>
<td align="left"><code>svm_c_*</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\nu\)</span>-classification/
regression<sup>c</sup></td>
<td align="left"><code>svm_nu_*</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\epsilon\)</span>-regression<sup>c</sup></td>
<td align="left"><code>svm_eps_*</code></td>
<td align="center"></td>
<td align="center"></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">linear kernel<sup>d</sup></td>
<td align="left"><code>svm_*_linear</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">polynomial kernel<sup>d</sup></td>
<td align="left"><code>svm_*_polynomial</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">radial kernel<sup>d</sup></td>
<td align="left"><code>svm_*_radial</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left">sigmoid kernel<sup>d</sup></td>
<td align="left"><code>svm_*_sigmoid</code></td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center">×</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<div id="configuration-options" class="section level2">
<h2>Configuration options</h2>
<p>Learners, their hyperparameters, and parallelisation options can be
specified using the tags or arguments below:</p>
<table>
<caption>Configuration options for model development.</caption>
<colgroup>
<col width="23%" />
<col width="50%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><strong>tag</strong> / <strong>argument</strong></th>
<th align="left"><strong>description</strong></th>
<th align="center"><strong>default</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>learner</code></td>
<td align="left">The desired learner. Multiple learners may be provided
at the same time. This setting has no default and must be provided.</td>
<td align="center">– (required)</td>
</tr>
<tr class="even">
<td align="center"><code>hyperparameter</code></td>
<td align="left">Each learner has one or more hyperparameters which can
be specified. Sequential model-based optimisation is used to identify
hyperparameter values from the data unless these are specifically
specified here. See the section on hyperparameter optimisation for more
details.</td>
<td align="center">– (optional)</td>
</tr>
<tr class="odd">
<td align="center"><code>parallel_model_development</code></td>
<td align="left">Enables parallel processing for model development.
Ignored if <code>parallel=FALSE</code>.</td>
<td align="center"><code>TRUE</code></td>
</tr>
</tbody>
</table>
</div>
<div id="generalised-linear-models" class="section level2">
<h2>Generalised linear models</h2>
<p>Generalised linear models (GLM) are easy to understand, use and
share. In many situations, GLM may be preferred over more complex models
as they are easier to report and validate. The most basic GLM is the
linear model. The linear model for an outcome variable <span class="math inline">\(y_j\)</span> and a single predictor variable <span class="math inline">\(x_j\)</span> for sample <span class="math inline">\(j\)</span> is:</p>
<p><span class="math display">\[y_j=\beta_0 + \beta_1 x_j +
\epsilon_j\]</span> Here, <span class="math inline">\(\beta_0\)</span>
and <span class="math inline">\(\beta_1\)</span> are both model
coefficients. <span class="math inline">\(\beta_0\)</span> is also
called the model intercept. <span class="math inline">\(\epsilon_j\)</span> is an error term that
describes the difference between the predicted and the actual response
for sample <span class="math inline">\(j\)</span>. When a linear model
is developed, the <span class="math inline">\(\beta\)</span>-coefficients are set in such manner
that the mean-squared error over the sample population is minimised.</p>
<p>The above model is a univariate model as it includes only a single
predictor. A multivariate linear model includes multiple predictors
<span class="math inline">\(\mathbf{X_j}\)</span> and is denoted as:</p>
<p><span class="math display">\[y_j= \mathbf{\beta}\mathbf{X_j}
+  \epsilon_j\]</span></p>
<p><span class="math inline">\(\mathbf{\beta}\mathbf{X_j}\)</span> is
the linear predictor. The GLM generalises this linear predictor through
a linking function <span class="math inline">\(g\)</span> <span class="citation">(Nelder and Wedderburn 1972)</span>:</p>
<p><span class="math display">\[y_j=g\left(\mathbf{\beta}\mathbf{X}\right) +
\epsilon_j\]</span></p>
<p>For example, binomial outcomes are commonly modelled using logistic
regression with the <code>logit</code> linking function. Multiple
linking functions are available in familiar and are detailed below.</p>
<div id="linear-models-for-binomial-outcomes" class="section level3">
<h3>Linear models for binomial outcomes</h3>
<p>Linear models for binomial outcomes derive from the
<code>stats</code> package which is part of the R core distribution
<span class="citation">(R Core Team 2019)</span>. Hyperparameters for
these models include linkage functions and are shown in the table
below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">linking function</td>
<td align="left"><code>family</code></td>
<td align="center"><code>logistic</code>, <code>probit</code>,
<code>loglog</code>, <code>cauchy</code></td>
<td align="center"><code>glm</code> only</td>
<td align="left">The linking function is not optimised when it is
specified, e.g. <code>glm_logistic</code>.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code>.
Instances with the majority class receive less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
</tbody>
</table>
</div>
<div id="linear-models-for-multinomial-outcomes" class="section level3">
<h3>Linear models for multinomial outcomes</h3>
<p>Prior to version 1.3.0 the linear model for multinomial outcomes was
implemented using the <code>VGAM::vglm</code> function <span class="citation">(T. W. Yee and Wild 1996; T. Yee 2010; Thomas W. Yee
2015)</span>. For better scalability, this has been replaced by the
<code>nnet::multinom</code> function <span class="citation">(Venables
and Ripley 2002)</span>. Hyperparameters for this model are shown in the
table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">linking function</td>
<td align="left"><code>family</code></td>
<td align="center"><code>multinomial</code></td>
<td align="center">no</td>
<td align="left">There is only one linking function available.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code>.
Instances with the majority class receive less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
</tbody>
</table>
</div>
<div id="linear-models-for-continuous-and-count-type-outcomes" class="section level3">
<h3>Linear models for continuous and count-type outcomes</h3>
<p>Linear models for continuous and count-type outcomes derive from the
<code>stats</code> package of the R core distribution <span class="citation">(R Core Team 2019)</span>. Hyperparameters for these
models include linkage functions and are shown in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">linking function</td>
<td align="left"><code>family</code></td>
<td align="center"><code>gaussian</code>, <code>log_gaussian</code>,
<code>inv_gaussian</code>, <code>poisson</code>,
<code>log_poisson</code></td>
<td align="center"><code>glm</code> only</td>
<td align="left">The linking function is not optimised when it is
specified, e.g. <code>glm_poisson</code>. <code>gaussian</code>,
<code>log_gaussian</code>, <code>inv_gaussian</code> linking functions
are not available for count-type outcomes.</td>
</tr>
</tbody>
</table>
</div>
<div id="linear-models-for-survival-outcomes" class="section level3">
<h3>Linear models for survival outcomes</h3>
<p>Linear models for survival outcomes are divided into semi-parametric
and parametric models. The semi-parametric Cox proportional hazards
model <span class="citation">(Cox 1972)</span> is based on the
<code>survival::coxph</code> function <span class="citation">(Therneau
and Grambsch 2000)</span>. Tied survival times are resolved using the
default method by <span class="citation">Efron (1977)</span>.</p>
<p>Various fully parametric models are also available, which differ in
the assumed distribution of the outcome, i.e. the linking function. The
parametric models are all based on <code>survival::survreg</code> <span class="citation">(Therneau and Grambsch 2000)</span>.</p>
<p>Hyperparameters for semi-parametric and parametric survival models
are shown in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">linking function</td>
<td align="left"><code>family</code></td>
<td align="center"><code>weibull</code>, <code>exponential</code>,
<code>gaussian</code>, <code>logistic</code>, <code>lognormal</code>,
<code>loglogistic</code></td>
<td align="center"><code>survival_regr</code> only</td>
<td align="left">The linking function is not optimised when it is
specified, e.g. <code>survival_regr_weibull</code>. The non-parametric
<code>cox</code> learner does not have a linking function.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="lasso-ridge-and-elastic-net-regression" class="section level2">
<h2>Lasso, ridge and elastic net regression</h2>
<p>Generalised linear models can be problematic as there is no inherent
limit to model complexity, which can easily lead to overfitting.
Penalised regression, or shrinkage, methods address this issue by
penalising model complexity.</p>
<p>Three shrinkage methods are implemented in <code>familiar</code>,
namely ridge regression, lasso and elastic net, which are all
implemented using the <code>glmnet</code> package <span class="citation">(Hastie, Tibshirani, and Friedman 2009; Simon et al.
2011)</span>. The set of hyperparameters is shown in the table below.
The optimal <code>lambda</code> parameter is determined by
cross-validation as part of the <code>glmnet::cv.glmnet</code> function,
and is not directly determined using hyperparameter optimisation.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">elastic net penalty</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,1\right]\)</span></td>
<td align="center">elastic net</td>
<td align="left">This penalty is fixed for ridge regression
(<code>alpha = 0</code>) and lasso (<code>alpha = 1</code>).</td>
</tr>
<tr class="odd">
<td align="left">optimal lambda</td>
<td align="left"><code>lambda_min</code></td>
<td align="center"><code>lambda.1se</code>, <code>lambda.min</code></td>
<td align="center">no</td>
<td align="left">Default is <code>lambda.min</code>.</td>
</tr>
<tr class="even">
<td align="left">number of CV folds</td>
<td align="left"><code>n_folds</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[3,n\right]\)</span></td>
<td align="center">no</td>
<td align="left">Default is <span class="math inline">\(3\)</span> if
<span class="math inline">\(n&lt;30\)</span>, <span class="math inline">\(\lfloor n/10\rfloor\)</span> if <span class="math inline">\(30\leq n \leq 200\)</span> and <span class="math inline">\(20\)</span> if <span class="math inline">\(n&gt;200\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code> for
<code>binomial</code> and <code>multinomial</code> outcomes, and
<code>none</code> otherwise. Instances with the majority class receive
less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">normalisation</td>
<td align="left"><code>normalise</code></td>
<td align="center"><code>FALSE</code>, <code>TRUE</code></td>
<td align="center">no</td>
<td align="left">Default is <code>FALSE</code>, as normalisation is part
of pre-processing in familiar.</td>
</tr>
</tbody>
</table>
</div>
<div id="boosted-generalised-linear-models-and-regression-trees" class="section level2">
<h2>Boosted generalised linear models and regression trees</h2>
<p>Boosting is a procedure which combines many weak learners to form a
more powerful panel <span class="citation">(Schapire 1990; Hastie,
Tibshirani, and Friedman 2009)</span>. Boosting learners are implemented
using the <code>mboost</code> package <span class="citation">(Bühlmann
and Hothorn 2007; Hothorn et al. 2010; Hofner, Boccuto, and Göker
2015)</span>. Both linear regression (<code>mboost::glmboost</code>) and
regression trees (<code>mboost::blackboost</code>) are implemented. The
hyperparameters are shown in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">family</td>
<td align="left"><code>family</code></td>
<td align="center"><code>logistic</code>, <code>probit</code>,
<code>bin_loglog</code>, <code>cauchy</code>, <code>log</code>,
<code>auc</code>, <code>gaussian</code>, <code>huber</code>,
<code>laplace</code>, <code>poisson</code>, <code>cox</code>,
<code>weibull</code>, <code>lognormal</code>, <code>surv_loglog</code>,
<code>gehan</code>, <code>cindex</code></td>
<td align="center">general tags</td>
<td align="left">The family is not optimised when it is specified,
e.g. <code>boosted_tree_gaussian</code>.</td>
</tr>
<tr class="odd">
<td align="left">boosting iterations</td>
<td align="left"><code>n_boost</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale, i.e. the actual input
value will be <span class="math inline">\(10^\texttt{n_boost}\)</span>.
The default range is <span class="math inline">\(\left[0,
3\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">learning rate</td>
<td align="left"><code>learning_rate</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[-\infty,0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale. The default range is
<span class="math inline">\(\left[-5,0\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code> for
<code>binomial</code> and <code>multinomial</code> outcomes, and
<code>none</code> otherwise. Instances with the majority class receive
less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">maximum tree depth</td>
<td align="left"><code>tree_depth</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">trees only</td>
<td align="left">Maximum depth to which trees are allowed to grow.</td>
</tr>
<tr class="even">
<td align="left">minimum sum of instance weight</td>
<td align="left"><code>min_child_weight</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,\infty\right)\)</span></td>
<td align="center">trees only</td>
<td align="left">Minimal instance weight required for further branch
partitioning, or the number of instances required in each node. This
parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale with a <span class="math inline">\(-1\)</span> offset. The default range is <span class="math inline">\(\left[0, 2\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">significance split threshold</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0.0,
1.0\right]\)</span></td>
<td align="center">trees only</td>
<td align="left">Minimum significance level for further splitting. The
default range is <span class="math inline">\(\left[10^{-6},
1.0\right]\)</span></td>
</tr>
</tbody>
</table>
<p>Optimising the number of boosting iterations may be slow as models
with a large number of boosting iterations take longer to learn and
assess. If this is an issue, the <code>n_boost</code> parameter should
be set to a smaller range, or provided with a fixed value.</p>
<p>Low learning rates may require an increased number of boosting
iterations.</p>
<p>Also note that hyperparameter optimisation time depends on the type
of family chosen, e.g. <code>cindex</code> is considerably slower than
<code>cox</code>.</p>
</div>
<div id="extreme-gradient-boosted-linear-models-and-trees" class="section level2">
<h2>Extreme gradient boosted linear models and trees</h2>
<p>Boosting is a procedure which combines many weak learners to form a
more powerful panel <span class="citation">(Schapire 1990; Hastie,
Tibshirani, and Friedman 2009)</span>. Extreme gradient boosting is a
gradient boosting implementation that was highly successful in several
machine learning competitions. Learners are implemented using the
<code>xgboost</code> package <span class="citation">(Chen and Guestrin
2016)</span>. Three types are implemented: regression based boosting
(<code>xgboost_lm</code>), regression-tree based boosting
(<code>xgboost_tree</code>) and regression-tree based boosting with
drop-outs (<code>xgboost_dart</code>).</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">family</td>
<td align="left"><code>learn_objective</code></td>
<td align="center"><code>gaussian,</code>
<code>continuous_logistic</code>, <code>multinomial_logistic</code>,
<code>binomial_logistic</code>, <code>poisson</code>,
<code>gamma</code>, <code>cox</code></td>
<td align="center">general tags</td>
<td align="left">The family is not optimised when it is specified,
e.g. <code>xgboost_lm_linear</code>.</td>
</tr>
<tr class="odd">
<td align="left">boosting iterations</td>
<td align="left"><code>n_boost</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,\infty \right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale, i.e. the actual value
will be <span class="math inline">\(10^\texttt{n_boost}\)</span>. The
default range is <span class="math inline">\(\left[0, 3
\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">learning rate</td>
<td align="left"><code>learning_rate</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left(-\infty,0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale. The default range is
<span class="math inline">\(\left[-5, 0 \right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">L1 regularisation</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[-6,
\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale with a <span class="math inline">\(10^{-6}\)</span> offset. The default range is
<span class="math inline">\(\left[-6, 3\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">L2 regularisation</td>
<td align="left"><code>lambda</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[-6,
\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale with a <span class="math inline">\(10^{-6}\)</span> offset. The default range is
<span class="math inline">\(\left[-6, 3\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code> for
<code>binomial</code> and <code>multinomial</code> outcomes, and
<code>none</code> otherwise. Instances with the majority class receive
less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">maximum tree depth</td>
<td align="left"><code>tree_depth</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,\infty\right)\)</span></td>
<td align="center"><code>gbtree</code>, <code>dart</code></td>
<td align="left">Maximum depth to which trees are allowed to grow. The
default range is <span class="math inline">\(\left[1,
10\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">subsampling fraction</td>
<td align="left"><code>sample_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0,
1.0\right]\)</span></td>
<td align="center"><code>gbtree</code>, <code>dart</code></td>
<td align="left">Fraction of available data that is used for to create a
single tree. The default range is <span class="math inline">\(\left[2 /
m, 1.0\right]\)</span>, with <span class="math inline">\(m\)</span> the
number of samples.</td>
</tr>
<tr class="odd">
<td align="left">minimum sum of instance weight</td>
<td align="left"><code>min_child_weight</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,\infty\right)\)</span></td>
<td align="center"><code>gbtree</code>, <code>dart</code></td>
<td align="left">Minimal instance weight required for further branch
partitioning, or the number of instances required in each node. This
parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale with a <span class="math inline">\(-1\)</span> offset. The default range is <span class="math inline">\(\left[0, 2\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">min. splitting error reduction</td>
<td align="left"><code>gamma</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[-6,
\infty\right)\)</span></td>
<td align="center"><code>gbtree</code>, <code>dart</code></td>
<td align="left">Minimum error reduction required to allow splitting.
This parameter is expressed on the <span class="math inline">\(\log_{10}\)</span> scale with a <span class="math inline">\(10^{-6}\)</span> offset. The default range is
<span class="math inline">\(\left[-6, 3\right]\)</span>.
<code>continuous</code> and <code>count</code>-type outcomes are
normalised to the <span class="math inline">\(\left[0, 1\right]\)</span>
range prior to model fitting to deal with scaling issues. Values are
converted back to the original scale after prediction.</td>
</tr>
<tr class="odd">
<td align="left">DART sampling algorithm</td>
<td align="left"><code>sample_type</code></td>
<td align="center"><code>uniform</code>, <code>weighted</code></td>
<td align="center"><code>dart</code></td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">drop-out rate</td>
<td align="left"><code>rate_drop</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in
\left[0,1\right)\)</span></td>
<td align="center"><code>dart</code></td>
<td align="left">–</td>
</tr>
</tbody>
</table>
</div>
<div id="random-forest-rfsrc" class="section level2">
<h2>Random forest (RFSRC)</h2>
<p>Random forests (explain) <span class="citation">(Breiman
2001)</span>. The random forest learner is implemented through the
<code>randomForestSRC</code> package, which provides a unified interface
for different types of forests <span class="citation">(Ishwaran et al.
2008, 2011)</span>.</p>
<p>An outcome variable that represents count-type data is first
transformed using a <span class="math inline">\(\log(x+1)\)</span>
transformation. Predicted responses are then transformed to the original
scale using the inverse <span class="math inline">\(\exp(x)-1\)</span>
transformation.</p>
<p>Hyperparameters for random forest learners are shown in the table
below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">number of trees</td>
<td align="left"><code>n_tree</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[0,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{2}\)</span> scale, i.e. the actual input
value will be <span class="math inline">\(2^\texttt{n_tree}\)</span>
<span class="citation">(Oshiro, Perez, and Baranauskas 2012)</span>. The
default range is <span class="math inline">\(\left[4,
10\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">subsampling fraction</td>
<td align="left"><code>sample_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Fraction of available data that is used for to create a
single tree. The default range is <span class="math inline">\(\left[2 /
m, 1.0\right]\)</span>, with <span class="math inline">\(m\)</span> the
number of samples.</td>
</tr>
<tr class="even">
<td align="left">number of features at each node</td>
<td align="left"><code>m_try</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0.0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Familiar ensures that there is always at least one
candidate feature.</td>
</tr>
<tr class="odd">
<td align="left">node size</td>
<td align="left"><code>node_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Minimum number of unique samples in terminal nodes. The
default range is <span class="math inline">\(\left[5, \lfloor m /
3\rfloor\right]\)</span>, with <span class="math inline">\(m\)</span>
the number of samples.</td>
</tr>
<tr class="even">
<td align="left">maximum tree depth</td>
<td align="left"><code>tree_depth</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Maximum depth to which trees are allowed to grow. The
default range is <span class="math inline">\(\left[1,
10\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">number of split points</td>
<td align="left"><code>n_split</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[0,
\infty\right)\)</span></td>
<td align="center">no</td>
<td align="left">By default, splitting is deterministic and has one
split point (<span class="math inline">\(0\)</span>).</td>
</tr>
<tr class="even">
<td align="left">splitting rule</td>
<td align="left"><code>split_rule</code></td>
<td align="center"><code>gini</code>, <code>auc</code>,
<code>entropy</code>, <code>mse</code>, <code>quantile.regr</code>,
<code>la.quantile.regr</code>, <code>logrank</code>,
<code>logrankscore</code>, <code>bs.gradient</code></td>
<td align="center">no</td>
<td align="left">Default splitting rules are <code>gini</code> for
<code>binomial</code> and <code>multinonial</code> outcomes,
<code>mse</code> for <code>continuous</code> and <code>count</code>
outcomes and <code>logrank</code> for <code>survival</code>
outcomes.</td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code> for
<code>binomial</code> and <code>multinomial</code> outcomes, and
<code>none</code> otherwise. Instances with the majority class receive
less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
</tbody>
</table>
<p>Note that optimising that optimising the number of trees can be slow,
as big forests take longer to construct and perform more computations
for predictions. Hence hyperparameter optimisation may be sped up by
limiting the range of the <code>n_tree</code> parameter, or setting a
single value.</p>
</div>
<div id="random-forest-ranger" class="section level2">
<h2>Random forest (ranger)</h2>
<p>The second implementation of random forests comes from the
<code>ranger</code> package <span class="citation">(Wright and Ziegler
2017)</span>. It is generally faster than the implementation in
<code>randomForestSRC</code> and allows for different splitting rules,
such as maximally selected rank statistics <span class="citation">(Lausen and Schumacher 1992; Wright, Dankowski, and
Ziegler 2017)</span> and concordance index-based splitting <span class="citation">(Schmid, Wright, and Ziegler 2016)</span>.
Hyperparameters for the random forest are shown in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">number of trees</td>
<td align="left"><code>n_tree</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[0,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">This parameter is expressed on the <span class="math inline">\(\log_{2}\)</span> scale, i.e. the actual input
value will be <span class="math inline">\(2^\texttt{n_tree}\)</span>
<span class="citation">(Oshiro, Perez, and Baranauskas 2012)</span>. The
default range is <span class="math inline">\(\left[4,
10\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">subsampling fraction</td>
<td align="left"><code>sample_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Fraction of available data that is used for to create a
single tree. The default range is <span class="math inline">\(\left[2 /
m, 1.0\right]\)</span>, with <span class="math inline">\(m\)</span> the
number of samples.</td>
</tr>
<tr class="even">
<td align="left">number of features at each node</td>
<td align="left"><code>m_try</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0.0,
1.0\right]\)</span></td>
<td align="center">yes</td>
<td align="left">Familiar ensures that there is always at least one
candidate feature.</td>
</tr>
<tr class="odd">
<td align="left">node size</td>
<td align="left"><code>node_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Minimum number of unique samples in terminal nodes. The
default range is <span class="math inline">\(\left[5, \lfloor m /
3\rfloor\right]\)</span>, with <span class="math inline">\(m\)</span>
the number of samples.</td>
</tr>
<tr class="even">
<td align="left">maximum tree depth</td>
<td align="left"><code>tree_depth</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,\infty\right)\)</span></td>
<td align="center">yes</td>
<td align="left">Maximum depth to which trees are allowed to grow. The
default range is <span class="math inline">\(\left[1,
10\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">splitting rule</td>
<td align="left"><code>split_rule</code></td>
<td align="center"><code>gini</code>, <code>extratrees</code>,
<code>variance</code>, <code>logrank</code>, <code>C</code>,
<code>maxstat</code></td>
<td align="center">no</td>
<td align="left">Default splitting rules are <code>gini</code> for
<code>binomial</code> and <code>multinomial</code> outcomes and
<code>maxstat</code> for <code>continuous</code>, <code>count</code> and
<code>survival</code> outcomes.</td>
</tr>
<tr class="even">
<td align="left">significance split threshold</td>
<td align="left"><code>alpha</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left(0.0,
1.0\right]\)</span></td>
<td align="center"><code>maxstat</code></td>
<td align="left">Minimum significance level for further splitting. The
default range is <span class="math inline">\(\left[10^{-6},
1.0\right]\)</span></td>
</tr>
<tr class="odd">
<td align="left">sample weighting</td>
<td align="left"><code>sample_weighting</code></td>
<td align="center"><code>inverse_number_of_samples</code>,
<code>effective_number_of_samples</code>, <code>none</code></td>
<td align="center">no</td>
<td align="left">Sample weighting allows for mitigating class
imbalances. The default is <code>inverse_number_of_samples</code> for
<code>binomial</code> and <code>multinomial</code> outcomes, and
<code>none</code> otherwise. Instances with the majority class receive
less weight.</td>
</tr>
<tr class="even">
<td align="left">beta</td>
<td align="left"><code>sample_weighting_beta</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[-6,-1\right]\)</span></td>
<td align="center"><code>effective_number_of_samples</code> only</td>
<td align="left">Specifies the <span class="math inline">\(\beta\)</span> parameter for effective number of
samples weighting <span class="citation">(Cui et al. 2019)</span>. It is
expressed on the <span class="math inline">\(\log_{10}\)</span> scale:
<span class="math inline">\(\beta=1-10^\texttt{beta}\)</span>.</td>
</tr>
</tbody>
</table>
<p>Note that optimising the number of trees can be slow, as big forests
take longer to construct and perform more computations for predictions.
Hence hyperparameter optimisation may be sped up by limiting the range
of the <code>n_tree</code> parameter or setting a single value.</p>
</div>
<div id="naive-bayes" class="section level2">
<h2>Naive Bayes</h2>
<p>The naive Bayes classifier uses Bayes rule to predict posterior
probabilities. The naive Bayes classifier uses the
<code>e1071::naiveBayes</code> function <span class="citation">(Meyer et
al. 2021)</span>. The hyperparameters of the classifier are shown in the
table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">laplace smoothing</td>
<td align="left"><code>laplace</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0.0,
\infty \right)\)</span></td>
<td align="center">yes</td>
<td align="left">The default range is <span class="math inline">\(\left[0, 10\right]\)</span>.</td>
</tr>
</tbody>
</table>
</div>
<div id="k-nearest-neighbours" class="section level2">
<h2><em>k</em>-nearest neighbours</h2>
<p><em>k</em>-nearest neighbours is a simple clustering algorithm that
classifies samples based on the classes of their <em>k</em> nearest
neighbours in feature space. The <code>e1071::gknn</code> function is
used to implement <em>k</em>-nearest neighbours <span class="citation">(Meyer et al. 2021)</span>. The hyperparameters are
shown in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">number of nearest neighbours</td>
<td align="left"><code>k</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,m\right]\)</span></td>
<td align="center">yes</td>
<td align="left">The default range is <span class="math inline">\(\left[1, \lceil 2 m^{1/3} \rceil \right]\)</span>,
with <span class="math inline">\(m\)</span> the number of samples.</td>
</tr>
<tr class="odd">
<td align="left">distance metric</td>
<td align="left"><code>distance_metric</code></td>
<td align="center">all metrics supported by
<code>proxy::dist</code></td>
<td align="center">yes</td>
<td align="left">The default set of metrics is <code>gower</code>,
<code>euclidean</code> and <code>manhattan</code>.</td>
</tr>
</tbody>
</table>
</div>
<div id="support-vector-machines" class="section level2">
<h2>Support vector machines</h2>
<p>Support vector machines were originally defined to find optimal
margins between classes for classification problems. Support vector
machines were popularized after the use of the kernel trick was
described. Using the kernel trick the calculations are performed in an
implicit high-dimensional feature space, which is considerably more
efficient than explicit calculations <span class="citation">(Boser,
Guyon, and Vapnik 1992)</span>.</p>
<p>Familiar implements SVM using <code>e1071::svm</code>. We tried
<code>kernlab::ksvm</code>, but this function would reproducibly freeze
during unit testing. By default, both features and outcome data are
scaled internally. This was used to derive the default hyperparameter
ranges specified in the table below.</p>
<table>
<colgroup>
<col width="24%" />
<col width="14%" />
<col width="19%" />
<col width="19%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>parameter</strong></th>
<th align="left"><strong>tag</strong></th>
<th align="center"><strong>values</strong></th>
<th align="center"><strong>optimised</strong></th>
<th align="left"><strong>comments</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">signature size</td>
<td align="left"><code>sign_size</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in
\left[1,n\right]\)</span></td>
<td align="center">yes</td>
<td align="left">–</td>
</tr>
<tr class="even">
<td align="left">SVM kernel</td>
<td align="left"><code>kernel</code></td>
<td align="center"><code>linear</code>, <code>polynomial</code>,
<code>radial</code>, <code>sigmoid</code></td>
<td align="center">no</td>
<td align="left">Default is <code>radial</code>, unless specified as
part of the learner’s name.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(C\)</span></td>
<td align="left"><code>c</code></td>
<td align="center"><span class="math inline">\(\mathbb{R}\)</span></td>
<td align="center">yes</td>
<td align="left">The cost for violation of constraints is expressed on a
<span class="math inline">\(\log_{10}\)</span> scale, i.e. the actual
value is <span class="math inline">\(10^\texttt{c}\)</span>. The default
range is <span class="math inline">\(\left[-5, 3\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\epsilon\)</span></td>
<td align="left"><code>epsilon</code></td>
<td align="center"><span class="math inline">\(\mathbb{R}\)</span></td>
<td align="center"><code>continuous</code> and <code>count</code>
outcomes</td>
<td align="left">The error tolerance <span class="math inline">\(\epsilon\)</span> is only used for regression SVM.
<span class="math inline">\(\epsilon\)</span> is expressed on a <span class="math inline">\(\log_{10}\)</span> scale. The default range is
<span class="math inline">\(\left[-5, 1\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\nu\)</span></td>
<td align="left"><code>nu</code></td>
<td align="center"><span class="math inline">\(\mathbb{R}\)</span></td>
<td align="center"><code>nu</code> SVM type</td>
<td align="left"><code>nu</code> provides upper bounds for the fraction
of training errors and lower bounds for the fraction of support vectors
<span class="citation">(Chang and Lin 2011)</span>. It is expressed on a
<span class="math inline">\(\log_{10}\)</span> scale. The default range
is <span class="math inline">\(\left[-5, 1\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">inverse kernel width</td>
<td align="left"><code>gamma</code></td>
<td align="center"><span class="math inline">\(\mathbb{R}\)</span></td>
<td align="center">non-linear kernels</td>
<td align="left">This parameter specifies the inverse of the kernel
width. It is expressed on the <span class="math inline">\(\log_{10}\)</span> scale. The default range is
<span class="math inline">\(\left[-9, 3\right]\)</span>.</td>
</tr>
<tr class="odd">
<td align="left">polynomial degree</td>
<td align="left"><code>degree</code></td>
<td align="center"><span class="math inline">\(\mathbb{Z} \in \left[1,
\infty\right)\)</span></td>
<td align="center">polynomial kernel</td>
<td align="left">The default range is <span class="math inline">\(\left[1, 5\right]\)</span>.</td>
</tr>
<tr class="even">
<td align="left">kernel offset</td>
<td align="left"><code>offset</code></td>
<td align="center"><span class="math inline">\(\mathbb{R} \in \left[0,
\infty\right)\)</span></td>
<td align="center">polynomial and sigmoid kernels</td>
<td align="left">Negative values are not allowed. The default range is
<span class="math inline">\(\left[0, 1 \right]\)</span>.</td>
</tr>
</tbody>
</table>
<p>Several types of SVM algorithms exist for classification and
regression. The following SVM types are implemented:</p>
<ul>
<li><span class="math inline">\(C\)</span>-classification:
<code>c</code></li>
<li><span class="math inline">\(\nu\)</span>-classification:
<code>nu</code></li>
<li><span class="math inline">\(\nu\)</span>-regression:
<code>nu</code></li>
<li><span class="math inline">\(\epsilon\)</span>-regression:
<code>eps</code></li>
</ul>
<p>The following kernels are implemented:</p>
<ul>
<li>Linear kernel: <code>linear</code></li>
<li>Polynomial kernel: <code>polynomial</code></li>
<li>Radial basis function kernel: <code>radial</code></li>
<li>Hyperbolic tangent (sigmoid) kernel: <code>sigmoid</code></li>
</ul>
</div>
<div id="hyperparameter-optimization" class="section level1">
<h1>Hyperparameter optimization</h1>
<p>Hyperparameter optimisation is conducted to select model parameters
that are more likely to lead to generalisable results. The main
hyperparameter optimisation framework used by familiar is based on
sequential model-based optimisation (SMBO) <span class="citation">(Hutter, Hoos, and Leyton-Brown 2011)</span>, but with
significant updates and extensions.</p>
<p>Overall, the following steps are conducted to optimise
hyperparameters for a given learner and dataset:</p>
<ul>
<li><p>A set of bootstraps of the development dataset is made. This
allows for training the models using the in-bag data and assessing
performance using out-of-bag data. This is independent of any bootstraps
that may have been defined as part of the experimental design. In case a
subsampling method is specified in the experimental design, the
corresponding internal development dataset is bootstrapped.</p></li>
<li><p>Variable importance is determined for each of the in-bag datasets
to avoid positively biasing the optimisation process through information
leakage.</p></li>
<li><p>An initial exploration of the hyperparameter space is conducted.
A model is trained using each hyperparameter set and a bootstrap
subsample. It is then assessed by computing an optimisation score based
on model performance in the out-of-bag, and optionally, in-bag
data.</p></li>
<li><p>Exploration of the hyperparameter space then continues by
iteratively comparing the best known hyperparameter set against
challenger hyperparameter sets in so-called intensify steps. The
challengers are selected by predicting the expected model performance
for a new hyperparameter set and computing its utility <span class="citation">(Shahriari et al. 2016)</span>. In addition, we predict
model run times, and actively prune potential slow challengers. Utility
is computed using an acquisition function, of which several are
available in familiar. The set of challenger hyperparameter sets may
moreover be iteratively pruned, dependent on the exploration
method.</p></li>
<li><p>Exploration stops if no significant improvement to model
performance could be established by further exploring the hyperparameter
space, the total number of iterations has been reached, or bootstraps
samples have been exhausted.</p></li>
<li><p>If the best set of hyperparameters does not lead to a model that
exceeds performance of a naive model, e.g. one that always predicts the
majority class or median value, a naive model is trained
instead.</p></li>
</ul>
<div id="predicting-run-time-of-model" class="section level2">
<h2>Predicting run time of model</h2>
<p>Models take a certain time to train. Familiar is actively measuring
this time during hyperparameter optimisation for two reasons: first, to
optimise assignment of jobs to parallel nodes; and secondly, to prune
potential challenger sets that produce models that tend to run longer
than the best-known model. The strictness increases with increased
exploration.</p>
<p>Let <span class="math inline">\(m\)</span> be the number of
bootstraps used to quantify the most visited hyperparameter sets, and
<span class="math inline">\(M\)</span> the total number of bootstrap
samples that can be visited. Then let <span class="math inline">\(t_{\texttt{opt}}\)</span> be the runtime of the
best known model. The maximum time that a challenger hyperparameter set
is allowed for training is then empirically set to:</p>
<p><span class="math display">\[t_{\texttt{max}} = \left(5 - 4
\frac{m}{M} \right) t_{\texttt{opt}} \]</span></p>
<p>Hence, <span class="math inline">\(t_{\texttt{max}}\)</span>
converges to <span class="math inline">\(t_{\texttt{opt}}\)</span> for
<span class="math inline">\(m \rightarrow M\)</span>.</p>
<p>If maximum runtime is relatively insignificant, i.e. <span class="math inline">\(t_{\texttt{max}} &lt; 10.0\)</span> seconds, a
threshold of <span class="math inline">\(t_{\texttt{max}}= 10.0\)</span>
is used.</p>
<p>If the maximum runtime is not known, i.e. none of the hyperparameter
sets evaluated so far produced a valid model, the maximum time threshold
is set to infinite. In effect, no hyperparameter sets are pruned based
on expected run time.</p>
<p>A random forest is trained to infer runtimes for (new) hyperparameter
sets, based on the runtimes observed for visited hyperparameter sets.
The random forest subsequently infers runtime for a challenger
hyperparameter set. The runtime estimate is compared against <span class="math inline">\(t_{\texttt{max}}\)</span>, and if it exceeds the
threshold, it is rejected and not evaluated.</p>
</div>
<div id="assessing-goodness-of-hyperparameter-sets" class="section level2">
<h2>Assessing goodness of hyperparameter sets</h2>
<p>A summary score <span class="math inline">\(s\)</span> determines how
good a set of hyperparameters <span class="math inline">\(\mathbf{x}\)</span> is. This score is computed in
two steps. First an objective score is computed to assess model
performance for a hyperparameter set in a bootstrap subsample.
Subsequently an optimisation score is computed within the subsample.
Finally a summary score is determined from the optimisation score over
all subsamples.</p>
<p>An objective score <span class="math inline">\(s&#39;&#39;\)</span>
is computed from the performance metric value for a specific
hyperparameter set for in-bag (IB; <span class="math inline">\(s&#39;&#39;_{\textrm{IB}}\)</span>) and out-of-bag
data (OOB; <span class="math inline">\(s&#39;&#39;_{\textrm{OOB}}\)</span>). The
objective score always lies in the interval <span class="math inline">\([-1.0, 1.0]\)</span>. An objective score of <span class="math inline">\(1.0\)</span> always indicates the best possible
score. A score of <span class="math inline">\(0.0\)</span> indicates
that the hyperparameter set leads to the same degree of performance as
the best educated guess, i.e. the majority class (for binomial and
multinomial outcomes), the median value (for continuous and count
outcomes), or tied risk or survival times (for survival outcomes).
Objective scores that are either missing or below <span class="math inline">\(-1.0\)</span> are truncated to the value of <span class="math inline">\(-1.0\)</span>. In case multiple metrics are used
to assess model performance, the mean of the respective objective scores
is used.</p>
<p>The optimisation scores <span class="math inline">\(s&#39;\)</span>
and summary score <span class="math inline">\(s\)</span> of each
hyperparameter set are subsequently computed using one of the following
functions, which can be set using the <code>optimisation_function</code>
parameter:</p>
<ol style="list-style-type: decimal">
<li><p><code>validation</code>, <code>max_validation</code> (default):
The optimisation score for each bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}}\)</span>. The
summary score <span class="math inline">\(s\)</span> is then the average
of optimisation scores <span class="math inline">\(s&#39;\)</span>. This
is a commonly used criterion that tries to maximise the score on the OOB
validation data.</p></li>
<li><p><code>balanced</code>: The optimisation score for each bootstrap
subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}} - \left|
s&#39;&#39;_{\textrm{OOB}} - s&#39;&#39;_{\textrm{IB}} \right|\)</span>.
The summary score <span class="math inline">\(s\)</span> is then the
average of optimisation scores <span class="math inline">\(s&#39;\)</span>. A variation on
<code>max_validation</code> with a penalty for differences in
performance between the IB and OOB data. The underlying idea is that a
good set of hyperparameters should lead to models that perform well on
both development and validation data.</p></li>
<li><p><code>strong_balance</code>: The optimisation score for each
bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}} - 2\left|
s&#39;&#39;_{\textrm{OOB}}-s&#39;&#39;_{\textrm{IB}} \right|\)</span>.
The summary score <span class="math inline">\(s\)</span> is then the
average of optimisation scores <span class="math inline">\(s&#39;\)</span>. A variant of
<code>balanced</code> with a stronger penalty term.</p></li>
<li><p><code>validation_minus_sd</code>: The optimisation score for each
bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}}\)</span>. The
summary score <span class="math inline">\(s\)</span> is then the average
of optimisation scores <span class="math inline">\(s&#39;\)</span> minus
the standard deviation of <span class="math inline">\(s&#39;\)</span>.
This penalises hyperparameter sets that lead to wide variance on OOB
data.</p></li>
<li><p><code>validation_25th_percentile</code>: The optimisation score
for each bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}}\)</span>. The
summary score <span class="math inline">\(s\)</span> is the 25th
percentile of optimisation scores <span class="math inline">\(s&#39;\)</span>. Like
<code>validation_minus_sd</code> this penalises hyperparameter sets that
lead to wide variance on OOB data.</p></li>
<li><p><code>model_estimate</code>: The optimisation score for each
bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}}\)</span>. The
model used to infer utility of new hyperparameter sets (see the next
section <a href="#predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a>) is used to predict
the expected optimisation score <span class="math inline">\(\mu(\mathbf{x})\)</span>. This score is used as
summary score <span class="math inline">\(s\)</span>. Not available for
random search.</p></li>
<li><p><code>model_estimate_minus_sd</code>: The optimisation score for
each bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}}\)</span>. The
model used to infer utility of new hyperparameter sets (see the next
section <a href="#predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a>) is used to predict
the expected optimisation score <span class="math inline">\(\mu(\mathbf{x})\)</span> and its standard
deviation <span class="math inline">\(\sigma(\mathbf{x})\)</span>. Then
<span class="math inline">\(s = \mu(\mathbf{x}) -
\sigma(\mathbf{x})\)</span>. This penalises hyperparameter sets for
which a large variance is expected. Not available for random
search.</p></li>
<li><p><code>model_balanced_estimate</code>: The optimisation score for
each bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}} - \left|
s&#39;&#39;_{\textrm{OOB}} - s&#39;&#39;_{\textrm{IB}} \right|\)</span>.
The model used to infer utility of new hyperparameter sets (see the next
section <a href="#predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a>) is used to predict
the expected optimisation score <span class="math inline">\(\mu(\mathbf{x})\)</span>. This score is used as
summary score <span class="math inline">\(s\)</span>. The method is
similar to <code>model_estimate</code>, but predicts the balanced score
instead of the validation score. Not available for random
search.</p></li>
<li><p><code>model_balanced_estimate_minus_sd</code>: The optimisation
score for each bootstrap subsample is <span class="math inline">\(s&#39;=s&#39;&#39;_{\textrm{OOB}} - \left|
s&#39;&#39;_{\textrm{OOB}} - s&#39;&#39;_{\textrm{IB}} \right|\)</span>.
The model used to infer utility of new hyperparameter sets (see the next
section <a href="#predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a>) is used to predict
the expected optimisation score <span class="math inline">\(\mu(\mathbf{x})\)</span> and its standard
deviation <span class="math inline">\(\sigma(\mathbf{x})\)</span>. Then
<span class="math inline">\(s = \mu(\mathbf{x}) -
\sigma(\mathbf{x})\)</span>. This penalises hyperparameter sets for
which a large variance is expected. The method is similar to
<code>model_estimate_minus_sd</code>, but predicts the balanced score
and its variance instead. Not available for random search.</p></li>
</ol>
<p>The summary score is then used to select the best known
hyperparameter set, i.e. the set that maximises the summary score.
Moreover, the optimisation scores are used to identify candidate
hyperparameter sets as described in the following section. The
optimisation function is set using the
<code>optimisation_function</code> argument.</p>
</div>
<div id="predicting-optimisation-score-for-new-hyperparameter-sets" class="section level2">
<h2>Predicting optimisation score for new hyperparameter sets</h2>
<p>Any point in the hyperparameter space has a single, scalar,
optimisation score value that is <em>a priori</em> unknown. During the
optimisation process, the algorithm samples from the hyperparameter
space by selecting hyperparameter sets and computing the optimisation
score for one or more bootstraps. For each hyperparameter set the
resulting scores are distributed around the actual value.</p>
<p>A key point of Bayesian optimisation is the ability to estimate the
usefulness or utility of new hyperparameter sets. This ability is
facilitated by modelling the optimisation score of new hyperparameter
sets using the optimisation scores of observed hyperparameter sets.</p>
<p>The following models can be used for this purpose:</p>
<ul>
<li><p><code>gaussian_process</code> (default): Creates a localised
approximate Gaussian deterministic Gaussian Process. This is implemented
using the <code>laGP</code> package <span class="citation">(Gramacy
2016)</span>.</p></li>
<li><p><code>bayesian_additive_regression_trees</code> or
<code>bart</code>: Uses Bayesian Additive Regression Trees for
inference. Unlike standard random forests, BART allows for estimating
posterior distributions directly and can extrapolate. BART is
implemented using the <code>BART</code> package <span class="citation">(Sparapani, Spanbauer, and McCulloch
2021)</span>.</p></li>
<li><p><code>random_forest</code>: Creates a random forest for
inference. A random forest was originally used in the SMBO algorithm by
Hutter et al. <span class="citation">(Hutter, Hoos, and Leyton-Brown
2011)</span>. A weakness of random forests is their lack of
extrapolation beyond observed values, which limits their usefulness in
exploiting promising areas of hyperparameter space somewhat.</p></li>
</ul>
<p>In addition, familiar can perform random search (<code>random</code>
or <code>random_search</code>). This forgoes the use of models to steer
optimisation. Instead, the hyperparameter space is sampled at
random.</p>
<p>The learner used to predict optimisation scores can be specified
using the <code>hyperparameter_learner</code> parameter.</p>
</div>
<div id="acquisition-functions-for-utility-of-hyperparameter-sets" class="section level2">
<h2>Acquisition functions for utility of hyperparameter sets</h2>
<p>The expected values and the posterior values of optimisation scores
from the models are used to compute utility of new hyperparameter sets
using an acquisition function. The following acquisition functions are
available in familiar <span class="citation">(Shahriari et al.
2016)</span>. Let <span class="math inline">\(\nu=f(\mathbf{x})\)</span>
be the posterior distribution of hyperparameter set <span class="math inline">\(\mathbf{x}\)</span>, and <span class="math inline">\(\tau\)</span> the best observed optimisation
score. Let also <span class="math inline">\(\mu(\mathbf{x})\)</span> and
<span class="math inline">\(\sigma(\mathbf{x})\)</span> be the sample
mean and sample standard deviation for set <span class="math inline">\(\mathbf{x}\)</span> (implicitly for round <span class="math inline">\(m\)</span>):</p>
<ul>
<li><p><code>improvement_probability</code>: The probability of
improvement quantifies the probability that the expected optimisation
score for a set <span class="math inline">\(\mathbf{x}\)</span> is
better than the best observed optimisation score <span class="math inline">\(\tau\)</span>:</p>
<p><span class="math display">\[\alpha(\mathbf{x}) = P(\nu &gt; \tau) =
\Phi \left(\frac{\mu(\mathbf{x}) - \tau}
{\sigma(\mathbf{x})}\right)\]</span></p>
<p>Here <span class="math inline">\(\Phi\)</span> is the cumulative
distribution function of a normal distribution. Note that this
acquisition function is typically prone to convergence to local minima,
although in our algorithm this may be mitigated by the alternating
random search strategy.</p></li>
<li><p><code>improvement_empirical_probability</code>: Similar to
<code>improvement_probability</code>, but based directly on optimisation
scores predicted by the individual decision trees:</p>
<p><span class="math display">\[\alpha(\mathbf{x}) = P(\nu &gt; \tau) =
\frac{1}{N} \sum_{i=1}^N \left[\nu_i &gt; \tau\right]\]</span></p>
<p>with <span class="math inline">\(\left[\ldots\right]\)</span>
denoting an Iverson bracket, which takes the value 1 if the condition
specified by <span class="math inline">\(\ldots\)</span> is true, and 0
otherwise.</p></li>
<li><p><code>expected_improvement</code> (default): Expected improvement
is based on an improvement function, and is be computed as follows:</p>
<p><span class="math display">\[\alpha(\mathbf{x})=(\mu(\mathbf{x})-\tau) \Phi
\left(\frac{\mu(\mathbf{x}) - \tau}{\sigma(\mathbf{x})}\right) +
\sigma(\mathbf{x}) \phi\left(\frac{\mu(\mathbf{x}) -
\tau}{\sigma(\mathbf{x})}\right)\]</span></p>
<p>with <span class="math inline">\(\phi\)</span> denoting the normal
probability density function. If <span class="math inline">\(\sigma(\mathbf{x}) = 0\)</span>, <span class="math inline">\(\alpha(\mathbf{x})=0\)</span>.</p></li>
<li><p><code>upper_confidence_bound</code>: This acquisition function is
based on the upper confidence bound of the distribution, and is defined
as <span class="citation">(Srinivas et al. 2012)</span>:</p>
<p><span class="math display">\[\alpha(\mathbf{x}, t)=\mu(\mathbf{x}) +
\beta_t \sigma(\mathbf{x})\]</span></p>
<p>Here the parameter <span class="math inline">\(\beta_t\)</span>
depends on the current round <span class="math inline">\(t\)</span>,
which in our implementation would be roughly equivalent to the maximum
number of bootstraps performed for any of the hyperparameter sets, minus
<span class="math inline">\(1\)</span>. The <span class="math inline">\(\beta_t\)</span> parameter is then defined as
<span class="math inline">\(\beta_t= 2
\log(|D|(t+1)^2\pi^2/6\delta)\)</span> <span class="citation">(Srinivas
et al. 2012)</span>. Here, <span class="math inline">\(\delta \in
[0,1]\)</span> and <span class="math inline">\(|D|\)</span> the
dimensionality of the hyperparameter space. For their experiments
Srinivas et al. used <span class="math inline">\(\delta=0.1\)</span>,
and noted that scaling down <span class="math inline">\(\beta_t\)</span>
by a factor <span class="math inline">\(5\)</span> improves the
algorithm. Hence, we use <span class="math inline">\(\beta_t=
\frac{2}{5} \log\left(\frac{5}{3}
|D|(t+1)^2\pi^2\right)\)</span>.</p></li>
<li><p><code>bayes_upper_confidence_bound</code>: Proposed by Kaufmann
et al. <span class="citation">(Kaufmann, Cappé, and Garivier
2012)</span>, the Bayesian upper confidence bound is defined as:</p>
<p><span class="math display">\[\alpha(\mathbf{x}, t)=Q
\left(1-\frac{1}{(t + 1) \log(n)^c}, \nu_t\right)\]</span></p>
<p>with <span class="math inline">\(Q\)</span> being a quantile
function. Note that we here deviate from the original definition by
substituting the original <span class="math inline">\(t\)</span> by
<span class="math inline">\(t+1\)</span>, and not constraining the
posterior distribution <span class="math inline">\(\nu\)</span> to a
known function. In the original, <span class="math inline">\(n\)</span>
defines the number of rounds, with <span class="math inline">\(t\)</span> the current round. In our algorithm,
this is interpreted as <span class="math inline">\(n\)</span> being the
total number of bootstraps, and <span class="math inline">\(t\)</span>
the maximum number of bootstraps already performed for any of the
hyperparameter sets. Hence our implementation is the same, aside from
the lack of constraint on the posterior distribution. Based on the
simulations and recommendations by Kaufmann et al. we choose parameter
<span class="math inline">\(c=0\)</span>.</p></li>
</ul>
<p>The acquisition function can be specified using the
<code>acquisition_function</code> parameter.</p>
</div>
<div id="exploring-challenger-sets" class="section level2">
<h2>Exploring challenger sets</h2>
<p>After selecting challenger hyperparameter sets, the optimisation
score of models trained using these hyperparameter sets are compared
against the best-known score in a run-off intensify step. During this
step, models will be trained on new bootstrap data, compared, and then
trained on another set of bootstraps, and so on. Pruning the set of
challenger hyperparameters after each iteration reduces computational
load. Familiar implements the following methods:</p>
<ul>
<li><p><code>single_shot</code> (default): The set of alternative
parameter sets is not pruned, and each intensify iteration contains only
a single (initial) intensification step that only uses a single
bootstrap. This is the exploration method that under default settings
does the least in-depth exploration, with the advantage of being fast.
Extensive internal tests show that this exploration method yields
similarly performant models as more extensive exploration
methods.</p></li>
<li><p><code>successive_halving</code>: The set of alternative parameter
sets is pruned by removing the worst performing half of the sets after
each step <span class="citation">(Jamieson and Talwalkar 2016)</span>.
The set of investigated parameter sets gets progressively
smaller.</p></li>
<li><p><code>stochastic_reject</code>: The set of alternative parameter
sets is pruned by comparing the performance of each parameter set with
that of the incumbent best parameter set using a paired Wilcoxon
test.</p></li>
<li><p><code>none</code>: The set of alternative parameter sets is not
pruned. Compared to <code>single_shot</code>, hyperparameter sets are
assessed using multiple bootstraps (<code>smbo_step_bootstraps</code> *
<code>smbo_intensify_steps</code>).</p></li>
</ul>
<p>The method used to steer exploration can be set using the
<code>exploration_method</code> parameter.</p>
</div>
<div id="providing-hyperparameters-manually" class="section level2">
<h2>Providing hyperparameters manually</h2>
<p>It is possible to set hyperparameters manually. This can be used to
change hyperparameters that are fixed by default, to set a fixed value
for randomised hyperparameters, or to provide a different search range
for randomised hyperparameters.</p>
<p>Hyperparameters can be provided using the <code>hyperparameter</code>
tag. For the <code>glm_logistic</code> learner an example tag may for
example look as follows:</p>
<pre><code>&lt;hyperparameter&gt;
  &lt;glm_logistic&gt;
    &lt;sign_size&gt;5&lt;/sign_size&gt;
  &lt;/glm_logistic&gt;
&lt;/hyperparameter&gt;</code></pre>
<p>Or as a nested list passed as the <code>hyperparameter</code>
argument to <code>summon_familiar</code>:</p>
<pre><code>hyperparameter = list(&quot;glm_logistic&quot;=list(&quot;sign_size&quot;=5))</code></pre>
<p>More than one value can be provided. The behaviour changes depending
on whether the hyperparameter is categorical or numeric variable. In
case of categorical hyperparameters, the provided values define the
search range. For numerical hyperparameters, providing two values sets
the bounds of the search range, whereas providing more than two values
will define the search range itself.</p>
</div>
<div id="configuration-options-for-hyperparameter-optimisation" class="section level2">
<h2>Configuration options for hyperparameter optimisation</h2>
<p>Hyperparameter optimization may be configured using the
tags/arguments in the table below.</p>
<table>
<caption>Configuration options for hyperparameter
optimisation.</caption>
<colgroup>
<col width="23%" />
<col width="50%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><strong>tag</strong> / <strong>argument</strong></th>
<th align="left"><strong>description</strong></th>
<th align="center"><strong>default</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>optimisation_bootstraps</code></td>
<td align="left">Maximum number of bootstraps created for
optimisation.</td>
<td align="center"><code>20</code></td>
</tr>
<tr class="even">
<td align="center"><code>optimisation_determine_vimp</code></td>
<td align="left">If <code>TRUE</code>, compute variable importance for
each bootstrap. If <code>FALSE</code> use variable importance computed
during the feature selection step.</td>
<td align="center"><code>TRUE</code></td>
</tr>
<tr class="odd">
<td align="center"><code>smbo_random_initialisation</code></td>
<td align="left">If <code>random</code> initial parameter sets are
generated randomly from default ranges. If <code>fixed</code> or
<code>fixed_subsample</code>, the initial parameter sets are based on a
grid in parameter space.</td>
<td align="center"><code>fixed_subsample</code></td>
</tr>
<tr class="even">
<td align="center"><code>smbo_n_random_sets</code></td>
<td align="left">Sets the number of hyperparameter sets drawn for
initialisation. Ignored if <code>smbo_random_initialisation</code> is
<code>fixed</code>. If <code>smbo_random_initialisation</code> is
<code>fixed_subsample</code>, the number of selected hyperparameters may
be lower.</td>
<td align="center"><code>100</code></td>
</tr>
<tr class="odd">
<td align="center"><code>max_smbo_iterations</code></td>
<td align="left">Maximum number of intensify iterations of the SMBO
algorithm.</td>
<td align="center"><code>20</code></td>
</tr>
<tr class="even">
<td align="center"><code>smbo_stop_convergent_iterations</code></td>
<td align="left">Number of subsequent convergent SMBO iterations
required to stop hyperparameter optimisation early.</td>
<td align="center"><code>3</code></td>
</tr>
<tr class="odd">
<td align="center"><code>smbo_stop_tolerance</code></td>
<td align="left">Tolerance for recent optimisation scores to determine
convergence.</td>
<td align="center"><code>0.1 * 1 / sqrt(n_samples)</code></td>
</tr>
<tr class="even">
<td align="center"><code>smbo_time_limit</code></td>
<td align="left">Time limit (in minutes) for the optimisation
process.</td>
<td align="center"><code>NULL</code></td>
</tr>
<tr class="odd">
<td align="center"><code>smbo_initial_bootstraps</code></td>
<td align="left">Number of bootstraps assessed initially.</td>
<td align="center"><code>1</code></td>
</tr>
<tr class="even">
<td align="center"><code>smbo_step_bootstraps</code></td>
<td align="left">Number of bootstraps used within each step of an
intensify iteration.</td>
<td align="center"><code>3</code></td>
</tr>
<tr class="odd">
<td align="center"><code>smbo_intensify_steps</code></td>
<td align="left">Number of intensify steps within each intensify
iteration.</td>
<td align="center"><code>5</code></td>
</tr>
<tr class="even">
<td align="center"><code>optimisation_metric</code></td>
<td align="left">The metric used for optimisation,
e.g. <code>auc</code>. See the vignette on performance metrics for
available options. More than one metric may be specified.</td>
<td align="center"><code>auc_roc</code> (<code>binomial</code>,
<code>multinomial</code>); <code>mse</code> (<code>continuous</code>);
<code>msle</code> (<code>count</code>); and
<code>concordance_index</code> (<code>survival</code>)</td>
</tr>
<tr class="odd">
<td align="center"><code>optimisation_function</code></td>
<td align="left">The optimisation function used (see <a href="#assessing-goodness-of-hyperparameter-sets">Assessing goodness of
hyperparameter sets</a>).</td>
<td align="center"><code>balanced</code></td>
</tr>
<tr class="even">
<td align="center"><code>hyperparameter_learner</code></td>
<td align="left">Learner used to predict optimisation scores for new
hyperparameter sets (see <a href="#predicting-optimisation-score-for-new-hyperparameter-sets">Predicting
optimisation score for new hyperparameter sets</a>).</td>
<td align="center"><code>gaussian_process</code></td>
</tr>
<tr class="odd">
<td align="center"><code>acquisition_function</code></td>
<td align="left">The function used to quantify utility of hyperparameter
sets (see <a href="#acquisition-functions-for-utility-of-hyperparameter-sets">Acquisition
functions for utility of hyperparameter sets</a>).</td>
<td align="center"><code>expected_improvement</code></td>
</tr>
<tr class="even">
<td align="center"><code>exploration_method</code></td>
<td align="left">Method used to explore challenger hyperparameter sets
(see <a href="#exploring-challenger-sets">Exploring challenger
sets</a>).</td>
<td align="center"><code>single_shot</code></td>
</tr>
<tr class="odd">
<td align="center"><code>smbo_stochastic_reject_p_value</code></td>
<td align="left">The p-value level for stochastic pruning.</td>
<td align="center"><code>0.05</code></td>
</tr>
<tr class="even">
<td align="center"><code>parallel_hyperparameter_optimisation</code></td>
<td align="left">Enables parallel processing for hyperparameter
optimisation. Ignored if <code>parallel=FALSE</code>. Can be
<code>outer</code> to process multiple optimisation processes
simultaneously.</td>
<td align="center"><code>TRUE</code></td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="model-recalibration" class="section level1">
<h1>Model recalibration</h1>
<p>Even though learners may be good at discrimination, model calibration
can be lacklustre. Some learners are therefore recalibrated as
follows:</p>
<ol style="list-style-type: decimal">
<li><p>3-fold cross-validation is performed. Responses are predicted
using a new model trained on each training fold.</p></li>
<li><p>Three recalibration models are trained using the responses for
the training folds as input <span class="citation">(Niculescu-Mizil and
Caruana 2005)</span>.</p></li>
<li><p>The three recalibration models are then applied to (new)
responses of the full model, and the resulting values averaged for each
sample.</p></li>
</ol>
<p>The following learners currently undergo recalibration:</p>
<table>
<colgroup>
<col width="32%" />
<col width="32%" />
<col width="35%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>learner</strong></th>
<th align="center"><strong>outcome</strong></th>
<th align="left"><strong>recalibration model</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>xgboost_lm</code>,
<code>xgboost_lm_logistic</code></td>
<td align="center">binomial, multinomial</td>
<td align="left"><code>glm_logistic</code></td>
</tr>
<tr class="even">
<td align="left"><code>xgboost_tree</code>,
<code>xgboost_tree_logistic</code></td>
<td align="center">binomial, multinomial</td>
<td align="left"><code>glm_logistic</code></td>
</tr>
<tr class="odd">
<td align="left"><code>xgboost_lm_cox</code></td>
<td align="center">survival</td>
<td align="left"><code>glm_cox</code></td>
</tr>
<tr class="even">
<td align="left"><code>xgboost_tree_cox</code></td>
<td align="center">survival</td>
<td align="left"><code>glm_cox</code></td>
</tr>
<tr class="odd">
<td align="left"><code>boosted_glm_cindex</code>,
<code>boosted_glm_gehan</code></td>
<td align="center">survival</td>
<td align="left"><code>glm_cox</code></td>
</tr>
<tr class="even">
<td align="left"><code>boosted_tree_cindex</code>,
<code>boosted_tree_gehan</code></td>
<td align="center">survival</td>
<td align="left"><code>glm_cox</code></td>
</tr>
</tbody>
</table>
<p>Familiar currently does not recalibrate models by inverting the model
calibration curves, but may do so in the future.</p>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Boser1992-nk" class="csl-entry">
Boser, Bernhard E, Isabelle M Guyon, and Vladimir N Vapnik. 1992.
<span>“A Training Algorithm for Optimal Margin Classifiers.”</span> In
<em>Proceedings of the Fifth Annual Workshop on Computational Learning
Theory</em>, 144–52. ACM.
</div>
<div id="ref-Breiman2001-ao" class="csl-entry">
Breiman, Leo. 2001. <span>“Random Forests.”</span> <em>Mach. Learn.</em>
45 (1): 5–32.
</div>
<div id="ref-Buhlmann2007-ku" class="csl-entry">
Bühlmann, Peter, and Torsten Hothorn. 2007. <span>“Boosting Algorithms:
Regularization, Prediction and Model Fitting.”</span> <em>Stat.
Sci.</em> 22 (4): 477–505.
</div>
<div id="ref-Chang2011-ck" class="csl-entry">
Chang, Chih-Chung, and Chih-Jen Lin. 2011. <span>“<span>LIBSVM</span>: A
Library for Support Vector Machines.”</span> <em>ACM Trans. Intell.
Syst. Technol.</em> 2 (3): 27:1–27.
</div>
<div id="ref-Chen2016-lo" class="csl-entry">
Chen, Tianqi, and Carlos Guestrin. 2016. <span>“<span>XGBoost</span>: A
Scalable Tree Boosting System.”</span> In <em>Proceedings of the 22nd
<span>ACM</span> <span>SIGKDD</span> International Conference on
Knowledge Discovery and Data Mining</em>, 785–94.
</div>
<div id="ref-Cox1972-fc" class="csl-entry">
Cox, D R. 1972. <span>“Regression Models and
<span>Life-Tables</span>.”</span> <em>J. R. Stat. Soc. Series B Stat.
Methodol.</em> 34 (2): 187–202.
</div>
<div id="ref-Cui2019-ys" class="csl-entry">
Cui, Yin, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
2019. <span>“Class-Balanced Loss Based on Effective Number of
Samples.”</span> In <em>2019 <span>IEEE/CVF</span> Conference on
Computer Vision and Pattern Recognition (<span>CVPR</span>)</em>,
9268–77. Long Beach, CA, USA: IEEE.
</div>
<div id="ref-Efron1977-ww" class="csl-entry">
Efron, Bradley. 1977. <span>“The Efficiency of Cox’s Likelihood Function
for Censored Data.”</span> <em>J. Am. Stat. Assoc.</em> 72 (359):
557–65.
</div>
<div id="ref-Gramacy2016-aa" class="csl-entry">
Gramacy, Robert B. 2016. <span>“<span class="nocase">laGP</span>:
Large-Scale Spatial Modeling via Local Approximate Gaussian Processes in
<span>R</span>.”</span> <em>Journal of Statistical Software</em> 72 (1):
1–46.
</div>
<div id="ref-Hastie2009-ed" class="csl-entry">
Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The
Elements of Statistical Learning: Data Mining, Inference, and
Prediction</em>. Second Edition. Springer Series in Statistics. New
York, NY, United States: Springer Science+Business Media, LLC.
</div>
<div id="ref-Hofner2015-tt" class="csl-entry">
Hofner, Benjamin, Luigi Boccuto, and Markus Göker. 2015.
<span>“Controlling False Discoveries in High-Dimensional Situations:
Boosting with Stability Selection.”</span> <em>BMC Bioinformatics</em>
16 (May): 144.
</div>
<div id="ref-Hothorn2010-cu" class="csl-entry">
Hothorn, Torsten, Peter Bühlmann, Thomas Kneib, Matthias Schmid, and
Benjamin Hofner. 2010. <span>“Model-Based Boosting 2.0.”</span> <em>J.
Mach. Learn. Res.</em> 11 (Aug): 2109–13.
</div>
<div id="ref-Hutter2011-ea" class="csl-entry">
Hutter, Frank, Holger H Hoos, and Kevin Leyton-Brown. 2011.
<span>“Sequential <span>Model-Based</span> Optimization for General
Algorithm Configuration.”</span> In <em>Learning and Intelligent
Optimization</em>, edited by Carlos A Coello Coello, 6683:507–23.
Lecture Notes in Computer Science. Berlin, Heidelberg: Springer Berlin
Heidelberg.
</div>
<div id="ref-Ishwaran2008-hz" class="csl-entry">
Ishwaran, Hemant, Udaya B Kogalur, Eugene H Blackstone, and Michael S
Lauer. 2008. <span>“Random Survival Forests.”</span> <em>Ann. Appl.
Stat.</em> 2 (3): 841–60.
</div>
<div id="ref-Ishwaran2011-gu" class="csl-entry">
Ishwaran, Hemant, Udaya B Kogalur, Xi Chen, and Andy J Minn. 2011.
<span>“Random Survival Forests for High-Dimensional Data.”</span>
<em>Stat. Anal. Data Min.</em> 4 (1): 115–32.
</div>
<div id="ref-Jamieson2016-fq" class="csl-entry">
Jamieson, Kevin, and Ameet Talwalkar. 2016. <span>“Non-Stochastic Best
Arm Identification and Hyperparameter Optimization.”</span> In
<em>Proceedings of the 19th International Conference on Artificial
Intelligence and Statistics</em>, edited by Arthur Gretton and Christian
C Robert, 51:240–48. Proceedings of Machine Learning Research. Cadiz,
Spain: PMLR.
</div>
<div id="ref-Kaufmann2012-kh" class="csl-entry">
Kaufmann, Emilie, Olivier Cappé, and Aurélien Garivier. 2012. <span>“On
Bayesian Upper Confidence Bounds for Bandit Problems.”</span> In
<em>Artificial Intelligence and Statistics</em>, 592–600.
</div>
<div id="ref-Lausen1992-qh" class="csl-entry">
Lausen, Berthold, and Martin Schumacher. 1992. <span>“Maximally Selected
Rank Statistics.”</span> <em>Biometrics</em> 48 (1): 73.
</div>
<div id="ref-Meyer2021-aq" class="csl-entry">
Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and
Friedrich Leisch. 2021. <em>E1071: Misc Functions of the Department of
Statistics, Probability Theory Group (Formerly: E1071), TU Wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>.
</div>
<div id="ref-Nelder1972-rs" class="csl-entry">
Nelder, J A, and R W M Wedderburn. 1972. <span>“Generalized Linear
Models.”</span> <em>J. R. Stat. Soc. Ser. A</em> 135 (3): 370–84.
</div>
<div id="ref-Niculescu-Mizil2005-kj" class="csl-entry">
Niculescu-Mizil, Alexandru, and Rich Caruana. 2005. <span>“Predicting
Good Probabilities with Supervised Learning.”</span> In <em>Proceedings
of the 22nd International Conference on Machine Learning</em>, 625–32.
ACM.
</div>
<div id="ref-Oshiro2012-mq" class="csl-entry">
Oshiro, Thais Mayumi, Pedro Santoro Perez, and José Augusto Baranauskas.
2012. <span>“How Many Trees in a Random Forest?”</span> In <em>Machine
Learning and Data Mining in Pattern Recognition</em>, 154–68. Springer
Berlin Heidelberg.
</div>
<div id="ref-rcore2018" class="csl-entry">
R Core Team. 2019. <em>R: A Language and Environment for Statistical
Computing</em>. Vienna, Austria: R Foundation for Statistical Computing.
<a href="https://www.R-project.org/">https://www.R-project.org/</a>.
</div>
<div id="ref-Schapire1990-vn" class="csl-entry">
Schapire, Robert E. 1990. <span>“The Strength of Weak
Learnability.”</span> <em>Mach. Learn.</em> 5 (2): 197–227.
</div>
<div id="ref-Schmid2016-ie" class="csl-entry">
Schmid, Matthias, Marvin N Wright, and Andreas Ziegler. 2016. <span>“On
the Use of Harrell’s <span>C</span> for Clinical Risk Prediction via
Random Survival Forests.”</span> <em>Expert Syst. Appl.</em> 63
(November): 450–59.
</div>
<div id="ref-Shahriari2016-kx" class="csl-entry">
Shahriari, B, K Swersky, Z Wang, R P Adams, and N de Freitas. 2016.
<span>“Taking the Human Out of the Loop: A Review of Bayesian
Optimization.”</span> <em>Proc. IEEE</em> 104 (1): 148–75.
</div>
<div id="ref-Simon2011-ih" class="csl-entry">
Simon, Noah, Jerome Friedman, Trevor Hastie, and Rob Tibshirani. 2011.
<span>“Regularization Paths for Cox’s Proportional Hazards Model via
Coordinate Descent.”</span> <em>J. Stat. Softw.</em> 39 (5): 1–13.
</div>
<div id="ref-Sparapani2021-aa" class="csl-entry">
Sparapani, Rodney, Charles Spanbauer, and Robert McCulloch. 2021.
<span>“Nonparametric Machine Learning and Efficient Computation with
<span>B</span>ayesian Additive Regression Trees: The <span>BART</span>
<span>R</span> Package.”</span> <em>Journal of Statistical Software</em>
97 (1): 1–66.
</div>
<div id="ref-Srinivas2012-hq" class="csl-entry">
Srinivas, N, A Krause, S M Kakade, and M W Seeger. 2012.
<span>“<span>Information-Theoretic</span> Regret Bounds for Gaussian
Process Optimization in the Bandit Setting.”</span> <em>IEEE Trans. Inf.
Theory</em> 58 (5): 3250–65.
</div>
<div id="ref-Therneau2000-jv" class="csl-entry">
Therneau, Terry M, and Patricia M Grambsch. 2000. <em>Modeling Survival
Data: Extending the Cox Model</em>. Statistics for Biology and Health.
New York: Springer Science &amp; Business Media.
</div>
<div id="ref-Venables2002-ma" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002. <em>Modern Applied Statistics
with s</em>. Fourth. New York: Springer. <a href="https://www.stats.ox.ac.uk/pub/MASS4/">https://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-Wright2017-sj" class="csl-entry">
Wright, Marvin N, Theresa Dankowski, and Andreas Ziegler. 2017.
<span>“Unbiased Split Variable Selection for Random Survival Forests
Using Maximally Selected Rank Statistics.”</span> <em>Stat. Med.</em> 36
(8): 1272–84.
</div>
<div id="ref-Wright2017-wc" class="csl-entry">
Wright, Marvin N, and Andreas Ziegler. 2017. <span>“Ranger : A Fast
Implementation of Random Forests for High Dimensional Data in c++ and
<span>R</span>.”</span> <em>J. Stat. Softw.</em> 77 (1).
</div>
<div id="ref-Yee1996-ql" class="csl-entry">
Yee, T W, and C J Wild. 1996. <span>“Vector Generalized Additive
Models.”</span> <em>J. R. Stat. Soc. Series B Stat. Methodol.</em> 58
(3): 481–93.
</div>
<div id="ref-Yee2010-rg" class="csl-entry">
Yee, Thomas. 2010. <span>“The <span>VGAM</span> Package for Categorical
Data Analysis.”</span> <em>Journal of Statistical Software</em> 32 (10):
1–34.
</div>
<div id="ref-Yee2015-bw" class="csl-entry">
Yee, Thomas W. 2015. <em>Vector Generalized Linear and Additive Models:
With an Implementation in <span>R</span></em>. Springer.
</div>
</div>
</div>

<div class="footer">
<br>
<a rel="license" href="https://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFgAAAAfCAMAAABUFvrSAAAAIGNIUk0AAHolAACAgwAA+f8AAIDpAAB1MAAA6mAAADqYAAAXb5JfxUYAAAAEZ0FNQQAAsY58+1GTAAAAAXNSR0IB2cksfwAAAW5QTFRF////////////////7+/v39/f1tXV09bS0tXS0tXR0dTR0dTQ0NTQ0NPPz9PPztLOztHNzdHNzdHMz8/PzdDMzNDMzNDLzM/Ly8/Ly8/Ky87Kys3Jyc3Jyc3IyMzIyMzHx8vHxsrGxsrFxcnFxcnExMnExMjDw8jDxMfDw8fCwsfCwcXAwMXAwMW/wMS/v8S+v8O+vsO+vsK9vcK9vcK8v7+/vMG8vMG7vMC8u8C7u8C6ur+6ur+5ub65ub64uL23t7y2tru1tbq0tLqztLmzs7iysrixsrexsbewsbawsLavsLWvr7Wur7SusLOvrrStrrOtr7KvrbOsrLKrr6+vq7Gqn6OenqCdn5+flpmWk5iTkZSRkZORj4+PiYyJhIaEhIWEgoWCgICAfX98fH98eXx5cHJvcHBwYGBgXV5dUFFQUFBQQ0RDQEBAPj8+NTY1MjMxMDAwKSkpKCkoICAgGxsbEBAQDg4ODQ4NAAAAlzoSDQAAAAN0Uk5TAAoO5yEBUwAAAvhJREFUeNq1lutX2kAQxWmXFDVGYy1EIjQ2VZDiu1CsRQQURYvV+qSKj6II8rANYOT+9z0JqIASo9Y5ydkP2f2d2Ts7d2N4jRcJgwEIBwO+SbdTFGw8ZzZz1n5BdLgnfLPBcCT6fW1jY3P78QEYEA76PWMu0W5lGbrNZGrrYNg+u+ga9fgVcmxtY/NJZAOCfs+IY4Bn6eN8RdlEJX9Ed1uFIfdnfzC8uBJbv5tyqqhMLKa0wQHPiEOwMInLW4Eu9xmzfdDtmQ0uLK3cSXmvBBTS6QJQ2tMC+8YcgpnOApAzSa83mZEBZIff2odGfYFQJNqc8s4VchQhhFA5XO1pgCddAxaFKyeNpBpxGSgNmwXXxMxcWE25fkkJGUIIoExESQPsFnkmC0gUuQmjBGQZq+j2BEKR5dUGLVLIvbkGkxxSrcHO92wCkIyENJL3u+2O8Zng/FJsvR5cRF0GFIqtwaKVvoTcSxrCKOOS7hPdXwLhxUYtUFC+Z6AKQgpoDRZ6joEkaYo4cMQKril/KLLcCE4TVYmqFmkNsK0rD9lIiDdXKCSrwwEhREae6Ve0WIiuPg3M0xVlW171BBe21CGjbLbSYR0c/To3H409TQquHTggREKZ8pbjEiRqqxxXtWjjRLdvLrzUAK4Vr5qwZvEsJsCrzExWF9Tk9gIm84e74BRyRN9xeyS4vkHSmg1yK4Wxt5yUIClDayn0t3SteLWq3RQvjQrN31O87e2dEiBl0tJDJmTrykImN8dtq6AOpIw8Y3OMf2s+bvptU+hJqFrc1yCfpmZDkWYX0mv0H9WWpvS2tH6w8z27e58JJVi7c2ImuNBkQvrBOOWZc0CqsyFKtU3+97OuaQBnXGe90RuTMvCHtpziuWCcmDvPm64m+t2vlmuq/YHqqwnGCcfs1l+mCcbSmgtSe8iDGQNnPEsnrq//fZrltXS4tk3oAOPvT2tPF91uMrXTDNv340JrjQ4hbsHAxeE0z1ksHD99eKFdl0dl/P//Cl+9EPcfS+yBAoqk3eUAAAAASUVORK5CYII=" /></a>
This work is licensed under a <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
Cite as: Alex Zwanenburg. familiar: Vignettes and Documentation (2021). <a href="https://github.com/alexzwanenburg/familiar">https://github.com/alexzwanenburg/familiar</a>
</div>


<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
